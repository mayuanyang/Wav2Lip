{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchvision librosa wandb matplotlib lpips mediapipe pytorch-msssim scikit-image piq realesrgan facenet_pytorch transformers torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o checkpoints/RealESRGAN_x4plus.pth https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerSyncnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "-----\n",
      "-----\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 37045250\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py:111: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py:129: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "Global Step: 2, Epoch: 1, CE Loss: 0.6621847748756409: : 1it [00:06,  6.84s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "Global Step: 3, Epoch: 2, CE Loss: 0.6852050423622131: : 1it [00:04,  4.06s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "Global Step: 4, Epoch: 3, CE Loss: 0.6925910711288452: : 1it [00:03,  3.98s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "Global Step: 5, Epoch: 4, CE Loss: 0.7062722444534302: : 1it [00:04,  4.10s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "Global Step: 6, Epoch: 5, CE Loss: 0.6918932795524597: : 1it [00:03,  3.99s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "Global Step: 7, Epoch: 6, CE Loss: 0.7363547086715698: : 1it [00:04,  4.01s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "Global Step: 8, Epoch: 7, CE Loss: 0.7177907228469849: : 1it [00:04,  4.05s/it]\n",
      "0it [00:00, ?it/s]LR 2e-06\n",
      "LR 2e-06\n",
      "^C\n",
      "0it [00:01, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 346, in <module>\n",
      "    train(device, model, train_data_loader, test_data_loader, optimizer,\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 130, in train\n",
      "    output, audio_embedding, face_embedding = model(x, mel)\n",
      "                                              ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/models/transformer_syncnet.py\", line 106, in forward\n",
      "    face_embedding = self.face_encoder(face_embedding)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1603, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/models/conv.py\", line 16, in forward\n",
      "    out = self.conv_block(x)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 458, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 454, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syncnet training resume\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000188400.pth \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerSyncnetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "-----\n",
      "-----\n",
      "total trainable params 35938242\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_v2_train.py:111: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "Global Step: 2, Epoch: 1, CE Loss: 2.309955358505249, LR: 0.0001: : 1it [00:08,  8.57s/it]\n",
      "Global Step: 3, Epoch: 2, CE Loss: 5.1704487800598145, LR: 0.0001: : 1it [00:06,  6.94s/it]\n",
      "Global Step: 4, Epoch: 3, CE Loss: 1.635107398033142, LR: 0.0001: : 1it [00:08,  8.39s/it]\n",
      "Global Step: 5, Epoch: 4, CE Loss: 1.3813518285751343, LR: 0.0001: : 1it [00:06,  6.50s/it]\n",
      "Global Step: 6, Epoch: 5, CE Loss: 1.7441076040267944, LR: 0.0001: : 1it [00:06,  6.51s/it]\n",
      "Global Step: 7, Epoch: 6, CE Loss: 1.5371503829956055, LR: 0.0001: : 1it [00:06,  6.83s/it]\n",
      "Global Step: 8, Epoch: 7, CE Loss: 0.6081779599189758, LR: 0.0001: : 1it [00:06,  6.38s/it]\n",
      "Global Step: 9, Epoch: 8, CE Loss: 1.4130114316940308, LR: 0.0001: : 1it [00:06,  6.27s/it]\n",
      "Global Step: 10, Epoch: 9, CE Loss: 1.2952107191085815, LR: 0.0001: : 1it [00:06,  6.39s/it]\n",
      "Global Step: 11, Epoch: 10, CE Loss: 1.146876573562622, LR: 0.0001: : 1it [00:06,  6.20s/it]\n",
      "Global Step: 12, Epoch: 11, CE Loss: 0.7353254556655884, LR: 0.0001: : 1it [00:06,  6.26s/it]\n",
      "Global Step: 13, Epoch: 12, CE Loss: 1.0918418169021606, LR: 0.0001: : 1it [00:06,  6.99s/it]\n",
      "0it [00:00, ?it/s]^C\n"
     ]
    }
   ],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_v2_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResSyncNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "-----\n",
      "-----\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 67011842\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_ressyncnet_train.py:101: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "0it [00:00, ?it/s]/Users/eddyma/DEV/Github/Wav2Lip/transformer_ressyncnet_train.py:130: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/amp/autocast_mode.py:265: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py:1640: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n",
      "Global Step: 2, Epoch: 1, CE Loss: 0.6844416856765747, LR: 0.0001: : 1it [00:05,  5.09s/it]\n",
      "Global Step: 3, Epoch: 2, CE Loss: 0.8077898621559143, LR: 0.0001: : 1it [00:03,  3.25s/it]\n",
      "Global Step: 4, Epoch: 3, CE Loss: 3.167924404144287, LR: 0.0001: : 1it [00:03,  3.24s/it]\n",
      "Global Step: 5, Epoch: 4, CE Loss: 2.0945658683776855, LR: 0.0001: : 1it [00:03,  3.14s/it]\n",
      "Global Step: 6, Epoch: 5, CE Loss: 0.6946808099746704, LR: 0.0001: : 1it [00:03,  3.15s/it]\n",
      "Global Step: 7, Epoch: 6, CE Loss: 0.9783343076705933, LR: 0.0001: : 1it [00:03,  3.12s/it]\n",
      "Global Step: 8, Epoch: 7, CE Loss: 0.7850728631019592, LR: 0.0001: : 1it [00:03,  3.39s/it]\n",
      "Global Step: 9, Epoch: 8, CE Loss: 0.7777519226074219, LR: 0.0001: : 1it [00:03,  3.08s/it]\n",
      "Global Step: 10, Epoch: 9, CE Loss: 0.9909075498580933, LR: 0.0001: : 1it [00:03,  3.09s/it]\n",
      "Global Step: 11, Epoch: 10, CE Loss: 0.7247993350028992, LR: 0.0001: : 1it [00:03,  3.11s/it]\n",
      "Global Step: 12, Epoch: 11, CE Loss: 1.0150365829467773, LR: 0.0001: : 1it [00:03,  3.07s/it]\n",
      "Global Step: 13, Epoch: 12, CE Loss: 0.92286616563797, LR: 0.0001: : 1it [00:03,  3.06s/it]\n",
      "Global Step: 14, Epoch: 13, CE Loss: 0.7113677859306335, LR: 0.0001: : 1it [00:03,  3.05s/it]\n",
      "Global Step: 15, Epoch: 14, CE Loss: 0.8471580147743225, LR: 0.0001: : 1it [00:03,  3.12s/it]\n",
      "Global Step: 16, Epoch: 15, CE Loss: 0.7525232434272766, LR: 0.0001: : 1it [00:03,  3.12s/it]\n",
      "Global Step: 17, Epoch: 16, CE Loss: 0.6677507758140564, LR: 0.0001: : 1it [00:02,  2.99s/it]\n",
      "Global Step: 18, Epoch: 17, CE Loss: 0.8460391759872437, LR: 0.0001: : 1it [00:03,  3.02s/it]\n",
      "Global Step: 19, Epoch: 18, CE Loss: 0.9602089524269104, LR: 0.0001: : 1it [00:03,  3.04s/it]\n",
      "Global Step: 20, Epoch: 19, CE Loss: 0.7448999285697937, LR: 0.0001: : 1it [00:03,  3.08s/it]\n",
      "Global Step: 21, Epoch: 20, CE Loss: 0.7370817065238953, LR: 0.0001: : 1it [00:03,  3.25s/it]\n",
      "Global Step: 22, Epoch: 21, CE Loss: 0.6934680938720703, LR: 0.0001: : 1it [00:03,  3.07s/it]\n",
      "Global Step: 23, Epoch: 22, CE Loss: 0.6338606476783752, LR: 0.0001: : 1it [00:02,  2.99s/it]\n",
      "Global Step: 24, Epoch: 23, CE Loss: 0.8095800280570984, LR: 0.0001: : 1it [00:02,  2.99s/it]\n",
      "Global Step: 25, Epoch: 24, CE Loss: 0.8209564685821533, LR: 0.0001: : 1it [00:03,  3.01s/it]\n",
      "Global Step: 26, Epoch: 25, CE Loss: 0.8232284784317017, LR: 0.0001: : 1it [00:03,  3.02s/it]\n",
      "Global Step: 27, Epoch: 26, CE Loss: 0.6864845156669617, LR: 0.0001: : 1it [00:02,  3.00s/it]\n",
      "Global Step: 28, Epoch: 27, CE Loss: 0.8964137434959412, LR: 0.0001: : 1it [00:03,  3.03s/it]\n",
      "Global Step: 29, Epoch: 28, CE Loss: 0.9654087424278259, LR: 0.0001: : 1it [00:03,  3.10s/it]\n",
      "0it [00:00, ?it/s]^C\n",
      "0it [00:00, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_ressyncnet_train.py\", line 345, in <module>\n",
      "    train(device, model, train_data_loader, test_data_loader, optimizer,\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_ressyncnet_train.py\", line 131, in train\n",
      "    output, audio_embedding, face_embedding = model(x, mel)\n",
      "                                              ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/models/transformer_ressyncnet.py\", line 89, in forward\n",
      "    face_embedding = self.face_encoder(face)    # (batch_size, 2048)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 285, in forward\n",
      "    return self._forward_impl(x)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 273, in _forward_impl\n",
      "    x = self.layer1(x)\n",
      "        ^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/resnet.py\", line 150, in forward\n",
      "    out = self.conv2(out)\n",
      "          ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1603, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 458, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 454, in _conv_forward\n",
      "    return F.conv2d(input, weight, bias, self.stride,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train ressyncnet from scrach\n",
    "!python transformer_ressyncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python transformer_ressyncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint/ressyncnet \\\n",
    "--checkpoint_path checkpoints/syncnet_checkpoint/ressyncnet/checkpoint_step000074000.pth \\\n",
    "--train_root local_train_files \\\n",
    "--use_wandb False \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientSyncNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train efficientsyncnet from scrach\n",
    "!python transformer_efficientsyncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/wav2lip_checkpoint/better-syncnet \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/better-syncnet/checkpoint_step000260800.pth \\\n",
    "--train_root local_train_files \\\n",
    "--num_of_unet_layers 0 \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip resume\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/wav2lip_checkpoint/no-blackout/ \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/ressyncnet/checkpoint_step000074000.pth \\\n",
    "--checkpoint_path checkpoints/wav2lip_checkpoint/no-blackout/checkpoint_step000033000.pth \\\n",
    "--train_root local_train_files/ \\\n",
    "--use_augmentation False \\\n",
    "--num_of_unet_layers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wav2lip 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip\n",
    "!python transformer_wav2lip384_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/wav2lip_checkpoint/better-syncnet \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/better-syncnet/checkpoint_step000260800.pth \\\n",
    "--train_root local_train_files \\\n",
    "--num_of_unet_layers 0 \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip resume\n",
    "!python transformer_wav2lip384_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/wav2lip_checkpoint/no-blackout/ \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/better-syncnet/checkpoint_step000260800.pth \\\n",
    "--checkpoint_path checkpoints/wav2lip_checkpoint/better-syncnet/checkpoint_step000000090.pth \\\n",
    "--train_root local_train_files/ \\\n",
    "--use_augmentation False \\\n",
    "--num_of_unet_layers 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training file\n",
    "import os\n",
    "\n",
    "def get_subfolders(directory):\n",
    "    return [os.path.join(directory, name) for name in os.listdir(directory)\n",
    "            if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "for subdir in get_subfolders('training_data'):\n",
    "  for subdir2 in get_subfolders(subdir):\n",
    "    for root, dirs, files in os.walk(subdir2):\n",
    "        print(subdir2)\n",
    "        # Extract the desired portion (last two parts of the path)\n",
    "        desired_portion = os.path.join(*subdir2.split(os.sep)[-2:])\n",
    "\n",
    "        # # Path to the output text file\n",
    "        output_file_path = \"output.txt\"\n",
    "\n",
    "        # # Append the extracted portion to the text file as a new line\n",
    "        with open(output_file_path, 'a') as f:\n",
    "             f.write(desired_portion + '\\n')\n",
    "\n",
    "        print(f\"Appended '{desired_portion}' to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_wav2lip_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA fine tuning syncnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_syncnet_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "from syncnet_dataset import Dataset\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_syncnet_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank update\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=['self_attn.out_proj'],  # Targeting the attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer encoder\n",
    "model.transformer_encoder = get_peft_model(model.transformer_encoder, lora_config)\n",
    "\n",
    "# Define optimizer for fine-tuning\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', 'training_data/')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=10, shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for step, (x, mel, y) in enumerate(train_data_loader):\n",
    "        # Your training loop here\n",
    "        optimizer.zero_grad()\n",
    "        output, audio_embedding, face_embedding = model(x, mel)\n",
    "        loss = cross_entropy_loss(output, y) #if (global_epoch // 50) % 2 == 0 else contrastive_loss2(a, v, y)\n",
    "        print('The loss', loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), \"lora_adapter_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_syncnet_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Load the LoRA adapter weights\n",
    "lora_weights = torch.load(\"lora_syncnet_weights.pth\")\n",
    "\n",
    "# Update only the LoRA weights (specific keys in the state dict)\n",
    "model.load_state_dict(lora_weights, strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora fine tune wav2lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import LoRAConv2d, ResUNet, LoRATransposeConv2d  # Assume you have a LoRA-wrapped Conv2d\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "checkpoint_path = \"checkpoints/wav2lip_checkpoint/no-blackout/checkpoint_step000033000.pth\"\n",
    "lora_checkpoint_point = \"checkpoints/wav2lip_lora/lora_wav2lip_eddy_weights.pth\"\n",
    "\n",
    "def apply_lora_to_model(model, lora_rank=4, lora_scaling=0.5):\n",
    "    num_of_layers = 500\n",
    "    acc = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if acc >= num_of_layers:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        if ( \"output_block.output_block\" in name.lower()):\n",
    "            \n",
    "            if isinstance(module, nn.Conv2d):\n",
    "              print('The module name', name)\n",
    "              # Create a LoRA-wrapped Conv2d layer\n",
    "              lora_module = LoRAConv2d(\n",
    "                  in_channels=module.in_channels,\n",
    "                  out_channels=module.out_channels,\n",
    "                  kernel_size=module.kernel_size,\n",
    "                  stride=module.stride,\n",
    "                  padding=module.padding,\n",
    "                  dilation=module.dilation,\n",
    "                  groups=module.groups,\n",
    "                  bias=(module.bias is not None),\n",
    "                  lora_rank=lora_rank,\n",
    "                  lora_scaling=lora_scaling\n",
    "              )\n",
    "              # Copy the weights from the original module\n",
    "              lora_module.conv.weight = module.weight\n",
    "              if module.bias is not None:\n",
    "                  lora_module.conv.bias = module.bias\n",
    "              # Replace the module in the model\n",
    "              parent_module = model\n",
    "              *parent_names, child_name = name.split('.')\n",
    "              for parent_name in parent_names:\n",
    "                  parent_module = getattr(parent_module, parent_name)\n",
    "              setattr(parent_module, child_name, lora_module)\n",
    "\n",
    "              acc += 1\n",
    "\n",
    "            # if isinstance(module, nn.ConvTranspose2d):\n",
    "            #   print('The tranpose module name', name)\n",
    "            #   # Create a LoRA-wrapped Conv2d layer\n",
    "            #   lora_module = LoRATransposeConv2d(\n",
    "            #       in_channels=module.in_channels,\n",
    "            #       out_channels=module.out_channels,\n",
    "            #       kernel_size=module.kernel_size,\n",
    "            #       stride=module.stride,\n",
    "            #       padding=module.padding,\n",
    "            #       lora_rank=lora_rank,\n",
    "            #       lora_scaling=lora_scaling\n",
    "            #   )\n",
    "            #   # Copy the weights from the original module\n",
    "            #   lora_module.conv.weight = module.weight\n",
    "            #   if module.bias is not None:\n",
    "            #       lora_module.conv.bias = module.bias\n",
    "            #   # Replace the module in the model\n",
    "            #   parent_module = model\n",
    "            #   *parent_names, child_name = name.split('.')\n",
    "            #   for parent_name in parent_names:\n",
    "            #       parent_module = getattr(parent_module, parent_name)\n",
    "            #   setattr(parent_module, child_name, lora_module)\n",
    "\n",
    "            #   acc += 1\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "\n",
    "from torch import optim\n",
    "from wav2lip_dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data as data_utils\n",
    "\n",
    "\n",
    "\n",
    "recon_loss = nn.L1Loss()\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = Dataset('train', 'preprocessed', 'lora_training_files', False)\n",
    "train_data_loader = data_utils.DataLoader(train_dataset, batch_size=10, shuffle=True,num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Instantiate your model\n",
    "model = ResUNet()\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=0.00001)\n",
    "\n",
    "load_wav2lip_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = apply_lora_to_model(model, lora_rank=4, lora_scaling=1)\n",
    "\n",
    "#print(model)\n",
    "\n",
    "# Freeze original model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze LoRA parameters\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRAConv2d):\n",
    "        module.lora_A.requires_grad = True\n",
    "        module.lora_B.requires_grad = True\n",
    "\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "prog_bar = tqdm(enumerate(train_data_loader))\n",
    "nepochs = 100\n",
    "epoch = 1\n",
    "while epoch <= nepochs:\n",
    "  #for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "  for step, (x, indiv_mels, mel, gt) in enumerate(train_data_loader):\n",
    "    model.train()\n",
    "    # Your training loop here\n",
    "    optimizer.zero_grad()\n",
    "    g =  model(indiv_mels, x)\n",
    "    loss = recon_loss(g, gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Step: {}, Epoch: {}, Sync Loss: {}, L1: {}'.format(step, epoch, 0, loss.item()))\n",
    "\n",
    "  epoch += 1\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), lora_checkpoint_point)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "model = Wav2Lip()\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=0.001)\n",
    "\n",
    "load_wav2lip_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "# Load the LoRA fine-tuned weights\n",
    "lora_params = torch.load(lora_checkpoint_point)\n",
    "\n",
    "# Load the LoRA weights into the original model without changing the original weights\n",
    "model.load_state_dict(lora_params, strict=False)\n",
    "\n",
    "val_dataset = Dataset('val', 'training_data')\n",
    "val_data_loader = data_utils.DataLoader(val_dataset, batch_size=1, shuffle=True,num_workers=0)\n",
    "\n",
    "for x, indiv_mels, mel, gt in val_data_loader:\n",
    "            if x.shape[0] == 1:\n",
    "              step += 1\n",
    "              model.eval()\n",
    "\n",
    "              # Move data to CUDA device\n",
    "              x = x.to(device)\n",
    "              gt = gt.to(device)\n",
    "              indiv_mels = indiv_mels.to(device)\n",
    "              mel = mel.to(device)\n",
    "\n",
    "              g = model(indiv_mels, x)\n",
    "              \n",
    "              l1loss = recon_loss(g, gt)\n",
    "              print('The eval L1 loss', l1loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py \\\n",
    "--checkpoint_path checkpoints/wav2lip_checkpoint/no-blackout/checkpoint_step000061000.pth \\\n",
    "--face input/eddy.mp4 \\\n",
    "--audio input/en_1.wav \\\n",
    "--outfile results/no-blackout/eddy_61000_2layers_en1_1.mp4 \\\n",
    "--model_layers 1 \\\n",
    "--use_ref_img True \\\n",
    "--use_esrgan False \\\n",
    "--iteration 1\n",
    "#--lora_checkpoint_path checkpoints/wav2lip_lora/lora_wav2lip_eddy_weights.pth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facemesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_all_file_paths(base_dir):\n",
    "    file_paths = []  # List to store all file paths\n",
    "\n",
    "    # Walk through each directory and subdirectory\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            # Construct full file path\n",
    "            full_path = os.path.join(root, file)\n",
    "            # Append the full path to the list\n",
    "            file_paths.append(full_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Define your base directory\n",
    "base_dir = 'training_data/'\n",
    "\n",
    "# Get all file paths\n",
    "all_file_paths = get_all_file_paths(base_dir)\n",
    "\n",
    "for fname in all_file_paths:\n",
    "  if fname.lower().endswith(\".jpg\") and not fname.lower().endswith(\"_landmarks.jpg\"):\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.resize(img, (192, 192))\n",
    "    result = face_mesh.process(img)\n",
    "        \n",
    "    # Initialize the landmark channel as a zero matrix\n",
    "    landmark_channel = np.zeros((192, 192), dtype=np.uint8)\n",
    "    output_fname = fname.replace('.jpg', '_landmarks.jpg')  # Modify this line based on your file naming preference\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "            \n",
    "      for face_landmarks in result.multi_face_landmarks:\n",
    "        for landmark in face_landmarks.landmark:\n",
    "          # Convert landmark to pixel coordinates\n",
    "          \n",
    "          x = min(int(math.floor(landmark.x * 192)), 191)\n",
    "          y = min(int(math.floor(landmark.y * 192)), 191)\n",
    "\n",
    "          # Mark the corresponding location in the landmark_channel\n",
    "          landmark_channel[y, x] = 255  # Mark as white pixel for the landmark\n",
    "          # Save the landmark channel as an image\n",
    "          cv2.imwrite(output_fname, landmark_channel)\n",
    "          \n",
    "    else:\n",
    "      print('No face', fname)\n",
    "      cv2.imwrite(output_fname, landmark_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'training_data/'\n",
    "\n",
    "# Get all file paths\n",
    "all_file_paths = get_all_file_paths(base_dir)\n",
    "\n",
    "for fname in all_file_paths:\n",
    "    if fname.lower().endswith(\"_landmarks.jpg\"):\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "            print(f\"Deleted: {fname}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "face_data = torch.randn(5, 15, 96, 192) \n",
    "audio_data = torch.randn(5, 1, 80, 16)\n",
    "\n",
    "y = model(face_data, audio_data)\n",
    "#make_dot(y, params=dict(model.named_parameters())).render(\"model_architecture_param\", format=\"png\")\n",
    "\n",
    "make_dot(y).render(\"simplified_syncnet_model\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "from models import Wav2Lip\n",
    "\n",
    "model = Wav2Lip(num_of_blocks=2)\n",
    "\n",
    "face_data = torch.randn(5, 9, 192, 192)\n",
    "audio_data = torch.randn(5, 1, 80, 16)\n",
    "\n",
    "y = model(audio_data, face_data)\n",
    "#make_dot(y, params=dict(model.named_parameters())).render(\"model_architecture_param\", format=\"png\")\n",
    "\n",
    "make_dot(y).render(\"simplified_wav2lip_model\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calculate_psnr(pred_img, target_img, max_pixel_value=255.0):\n",
    "    \"\"\"\n",
    "    Calculate the PSNR between two images.\n",
    "    \n",
    "    Args:\n",
    "        pred_img (torch.Tensor): Generated image tensor of shape (batch, channels, height, width)\n",
    "        target_img (torch.Tensor): Ground truth image tensor of shape (batch, channels, height, width)\n",
    "        max_pixel_value (float): Maximum pixel value (255 for 8-bit images)\n",
    "    \n",
    "    Returns:\n",
    "        psnr (float): Peak Signal-to-Noise Ratio value\n",
    "    \"\"\"\n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    mse = F.mse_loss(pred_img, target_img)\n",
    "    \n",
    "    if mse == 0:\n",
    "        return float('inf')  # If MSE is zero, PSNR is infinite\n",
    "\n",
    "    psnr = 20 * torch.log10(torch.tensor(max_pixel_value)) - 10 * torch.log10(mse)\n",
    "    \n",
    "    return psnr.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.0\n",
      "The length of blocks 1\n",
      "Load checkpoint from: checkpoints/wav2lip_checkpoint/no-blackout/checkpoint_step000033000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_16106/3224025920.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PSNR(higher the better): 33.05471420288086 dB, LIPIS(lower the better): 0.12688323855400085, MS-SSIM(higher the better): 0.9544532299041748, BRISQUE(lower the better): 21.69061279296875\n",
      "PSNR(higher the better): 31.519603729248047 dB, LIPIS(lower the better): 0.15809759497642517, MS-SSIM(higher the better): 0.9503058791160583, BRISQUE(lower the better): 19.38397216796875\n",
      "PSNR(higher the better): 32.61578369140625 dB, LIPIS(lower the better): 0.1630537360906601, MS-SSIM(higher the better): 0.9416341781616211, BRISQUE(lower the better): 20.14080810546875\n",
      "PSNR(higher the better): 31.34691619873047 dB, LIPIS(lower the better): 0.17495858669281006, MS-SSIM(higher the better): 0.9398446679115295, BRISQUE(lower the better): 22.03924560546875\n",
      "PSNR(higher the better): 31.315631866455078 dB, LIPIS(lower the better): 0.1902342438697815, MS-SSIM(higher the better): 0.950167715549469, BRISQUE(lower the better): 23.80682373046875\n",
      "PSNR(higher the better): 31.769521713256836 dB, LIPIS(lower the better): 0.17867882549762726, MS-SSIM(higher the better): 0.9503543376922607, BRISQUE(lower the better): 19.92498779296875\n",
      "PSNR(higher the better): 31.044668197631836 dB, LIPIS(lower the better): 0.20250067114830017, MS-SSIM(higher the better): 0.9436677098274231, BRISQUE(lower the better): 17.64276123046875\n",
      "PSNR(higher the better): 32.573814392089844 dB, LIPIS(lower the better): 0.19679878652095795, MS-SSIM(higher the better): 0.9377005100250244, BRISQUE(lower the better): 21.59002685546875\n",
      "PSNR(higher the better): 32.38163757324219 dB, LIPIS(lower the better): 0.19826999306678772, MS-SSIM(higher the better): 0.9399349093437195, BRISQUE(lower the better): 22.42010498046875\n",
      "PSNR(higher the better): 32.3515739440918 dB, LIPIS(lower the better): 0.14588133990764618, MS-SSIM(higher the better): 0.9445939660072327, BRISQUE(lower the better): 21.20526123046875\n",
      "PSNR(higher the better): 30.57761001586914 dB, LIPIS(lower the better): 0.20357131958007812, MS-SSIM(higher the better): 0.9337876439094543, BRISQUE(lower the better): 20.55682373046875\n",
      "PSNR(higher the better): 30.842330932617188 dB, LIPIS(lower the better): 0.18894198536872864, MS-SSIM(higher the better): 0.9411165714263916, BRISQUE(lower the better): 17.65643310546875\n",
      "PSNR(higher the better): 30.63486099243164 dB, LIPIS(lower the better): 0.2058137059211731, MS-SSIM(higher the better): 0.9335337281227112, BRISQUE(lower the better): 21.94744873046875\n",
      "PSNR(higher the better): 32.83073043823242 dB, LIPIS(lower the better): 0.13758239150047302, MS-SSIM(higher the better): 0.9459202885627747, BRISQUE(lower the better): 19.50994873046875\n",
      "PSNR(higher the better): 32.673492431640625 dB, LIPIS(lower the better): 0.14397883415222168, MS-SSIM(higher the better): 0.9437659382820129, BRISQUE(lower the better): 25.41522216796875\n",
      "PSNR(higher the better): 30.633413314819336 dB, LIPIS(lower the better): 0.2041787952184677, MS-SSIM(higher the better): 0.9344736933708191, BRISQUE(lower the better): 18.35272216796875\n",
      "PSNR(higher the better): 32.14190673828125 dB, LIPIS(lower the better): 0.19674548506736755, MS-SSIM(higher the better): 0.9398062229156494, BRISQUE(lower the better): 22.61053466796875\n",
      "PSNR(higher the better): 32.96028137207031 dB, LIPIS(lower the better): 0.142234206199646, MS-SSIM(higher the better): 0.9429717063903809, BRISQUE(lower the better): 22.73944091796875\n",
      "PSNR(higher the better): 30.322763442993164 dB, LIPIS(lower the better): 0.21293401718139648, MS-SSIM(higher the better): 0.9331545829772949, BRISQUE(lower the better): 17.00018310546875\n",
      "PSNR(higher the better): 30.41153335571289 dB, LIPIS(lower the better): 0.20007753372192383, MS-SSIM(higher the better): 0.946326494216919, BRISQUE(lower the better): 18.25994873046875\n",
      "The max PSNR: 33.05471420288086 dB, LIPIS: 0.12688323855400085, MS-SSIM: 0.9544532299041748, BRISQUE: 17.00018310546875\n",
      "The avg PSNR: 31.700139427185057 dB, LIPIS: 0.14194731414318085, MS-SSIM: 0.9270713925361633, BRISQUE: 18.352916717529297\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from wav2lip_dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data as data_utils\n",
    "from models import ResUNet\n",
    "import lpips\n",
    "import torch\n",
    "import piq\n",
    "from pytorch_msssim import ms_ssim, ssim\n",
    "import cv2\n",
    "\n",
    "\n",
    "print(piq.__version__)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dataset_path = 'metrics'\n",
    "training_files_path = 'metrics_files'\n",
    "\n",
    "folder_name = \"no-blackout\"\n",
    "\n",
    "# dataset_path = 'training_data'\n",
    "# training_files_path = 'local_train_files'\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', dataset_path, training_files_path, False)\n",
    "train_data_loader = data_utils.DataLoader(train_dataset, batch_size=4, shuffle=True,num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Instantiate your model\n",
    "#checkpoint_path = \"checkpoints/wav2lip_checkpoint/more-ref/checkpoint_step000116000.pth\"\n",
    "#checkpoint_path = \"checkpoints/wav2lip_checkpoint/more-ref-2layers/checkpoint_step000182000.pth\"\n",
    "\n",
    "#checkpoint_path = \"checkpoints/wav2lip_checkpoint/more-ref-3layers/checkpoint_step000505000.pth\"\n",
    "checkpoint_path = f\"checkpoints/wav2lip_checkpoint/{folder_name}/checkpoint_step000033000.pth\"\n",
    "\n",
    "\n",
    "model = ResUNet(1)\n",
    "\n",
    "load_wav2lip_checkpoint(checkpoint_path, model, None, reset_optimizer=True)\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net='vgg').to(device)  # You can choose 'alex', 'vgg', or 'squeeze'\n",
    "\n",
    "prog_bar = tqdm(enumerate(train_data_loader))\n",
    "\n",
    "iteration = 20\n",
    "total_psnr = 0\n",
    "total_lpips = 0\n",
    "total_ssim = 0\n",
    "total_brisque = 0\n",
    "\n",
    "global_loipses = []\n",
    "global_ssim = []\n",
    "global_brisque = []\n",
    "\n",
    "psnr_max = 0.0\n",
    "ssim_max = 0.0\n",
    "lipis_min = 10.0\n",
    "brisque_min = 100.0        \n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for x in range(iteration):\n",
    "    for step, (x, indiv_mels, mel, gt) in enumerate(train_data_loader):\n",
    "      \n",
    "      g =  model(indiv_mels, x)\n",
    "      #g = torch.clamp(g, 0, 1)\n",
    "      \n",
    "      img = (g.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255).astype(np.uint8)  # Shape becomes [1, 5, 192, 192, 3]\n",
    "      gt2 = (gt.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255).astype(np.uint8)  # Shape becomes [1, 5, 192, 192, 3]\n",
    "\n",
    "      #print(\"img min:\", img.min().item(), \"img max:\", img.max().item())\n",
    "\n",
    "      # Step 2: Loop through batch and time dimensions to save each frame individually\n",
    "      for batch_idx, batch in enumerate(img):\n",
    "          for t, frame in enumerate(batch):\n",
    "              # Save each frame with unique filename\n",
    "              filename = f'checkpoints/wav2lip_checkpoint/{folder_name}/{batch_idx}_{t}.jpg'\n",
    "              cv2.imwrite(filename, frame)\n",
    "              #print(f\"Saved {filename}\")\n",
    "      \n",
    "      # Calculate PSNR\n",
    "      p = torch.tensor((g.detach().cpu().numpy() * 255).astype(np.uint8)).float()\n",
    "      t = torch.tensor((gt.detach().cpu().numpy() * 255).astype(np.uint8)).float()\n",
    "      psnr_value = calculate_psnr(p, t)\n",
    "      total_psnr += psnr_value\n",
    "\n",
    "      num_of_frames = g.shape[2]\n",
    "      disc_losses = []\n",
    "      ssim_losses = []\n",
    "      brisque_scores = []\n",
    "      \n",
    "\n",
    "      for i in range(num_of_frames):\n",
    "        gen_frame = g[:, :, i, :, :]  # Shape: [batch_size, 3, 192, 192]\n",
    "        gt_frame = gt[:, :, i, :, :]    # Shape: [batch_size, 3, 192, 192]\n",
    "        \n",
    "        lpips_f_loss = lpips_loss(gen_frame.to(device), gt_frame.to(device))\n",
    "        disc_losses.append(lpips_f_loss)\n",
    "\n",
    "        total_lpips += lpips_f_loss\n",
    "\n",
    "        # Calculate MS-SSIM\n",
    "        ms_ssim_value = ms_ssim(gen_frame, gt_frame, data_range=1.0)\n",
    "        ssim_losses.append(ms_ssim_value)\n",
    "        total_ssim += ms_ssim_value\n",
    "\n",
    "        score = piq.brisque(gen_frame)\n",
    "        brisque_scores.append(score)\n",
    "        total_brisque += score\n",
    "      \n",
    "      avg_lpips = torch.mean(torch.stack(disc_losses))\n",
    "      avg_ssim = torch.mean(torch.stack(ssim_losses))\n",
    "      avg_brisque_score = torch.mean(torch.stack(brisque_scores))\n",
    "      \n",
    "      global_loipses.append(avg_lpips)\n",
    "      global_ssim.append(avg_ssim)\n",
    "      global_brisque.append(avg_brisque_score)\n",
    "      \n",
    "      lpips_loss_value = torch.min(torch.stack(disc_losses))\n",
    "      ssim_value = torch.max(torch.stack(ssim_losses))\n",
    "      brisque_score = torch.min(torch.stack(brisque_scores))\n",
    "\n",
    "      if psnr_value > psnr_max:\n",
    "        psnr_max = psnr_value\n",
    "\n",
    "      if lpips_loss_value < lipis_min:\n",
    "        lipis_min = lpips_loss_value\n",
    "\n",
    "      if ssim_value > ssim_max:\n",
    "        ssim_max = ssim_value\n",
    "\n",
    "      if brisque_score < brisque_min:\n",
    "        brisque_min = brisque_score\n",
    "\n",
    "      print(f\"PSNR(higher the better): {psnr_value} dB, LIPIS(lower the better): {lpips_loss_value}, MS-SSIM(higher the better): {ssim_value}, BRISQUE(lower the better): {brisque_score}\")\n",
    "\n",
    "print(f\"The max PSNR: {psnr_max} dB, LIPIS: {lipis_min}, MS-SSIM: {ssim_max}, BRISQUE: {brisque_min}\")\n",
    "print(f\"The avg PSNR: {total_psnr/iteration} dB, LIPIS: {torch.min(torch.stack(global_loipses))}, MS-SSIM: {torch.min(torch.stack(global_ssim))}, BRISQUE: {torch.min(torch.stack(global_brisque))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the original model and its weights\n",
    "model = ResUNet(3)\n",
    "load_wav2lip_checkpoint('checkpoints/wav2lip_checkpoint/more-ref-3layers/checkpoint_step000505000.pth', model, None, reset_optimizer=True)\n",
    "\n",
    "# Load the fine-tuned model and its weights\n",
    "# fine_tuned_model = ResUNet(3)  # Instantiate the model class again if needed\n",
    "# load_wav2lip_checkpoint('checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000552500.pth', fine_tuned_model, None, reset_optimizer=True)\n",
    "\n",
    "# Transfer weights for frozen layers from the original model\n",
    "# for name, param in fine_tuned_model.named_parameters():\n",
    "#     if name in model.state_dict() and 'face_enhancer' in name:\n",
    "#         print('Copying', name)\n",
    "#         model.state_dict()[name].copy_(param)\n",
    "\n",
    "# Save the combined model\n",
    "torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": None,\n",
    "        \"global_step\": 505000,\n",
    "        \"global_epoch\": 100,\n",
    "    }, 'checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000505000_striped.pth')\n",
    "\n",
    "#torch.save(fine_tuned_model.state_dict(), \"checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000505000_combined.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

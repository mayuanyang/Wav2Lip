{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchvision librosa wandb matplotlib lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syncnet training resume\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth \\\n",
    "--use_wandb False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 68442979\n",
      "Load checkpoint from: checkpoints/checkpoint_step000000000.pth\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py\", line 391, in <module>\n",
      "    load_checkpoint(args.checkpoint_path, model, optimizer, reset_optimizer=False)\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py\", line 343, in load_checkpoint\n",
      "    checkpoint = _load(path)\n",
      "                 ^^^^^^^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py\", line 334, in _load\n",
      "    checkpoint = torch.load(checkpoint_path,\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/serialization.py\", line 1065, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/serialization.py\", line 468, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/serialization.py\", line 449, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "                     ^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/checkpoint_step000000000.pth'\n"
     ]
    }
   ],
   "source": [
    "# Train wav2lip resume\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth \\\n",
    "--checkpoint_path checkpoints/checkpoint_step000000001.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training file\n",
    "import os\n",
    "\n",
    "def get_subfolders(directory):\n",
    "    return [os.path.join(directory, name) for name in os.listdir(directory)\n",
    "            if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "for subdir in get_subfolders('training_data'):\n",
    "  for subdir2 in get_subfolders(subdir):\n",
    "    for root, dirs, files in os.walk(subdir2):\n",
    "        print(subdir2)\n",
    "        # Extract the desired portion (last two parts of the path)\n",
    "        desired_portion = os.path.join(*subdir2.split(os.sep)[-2:])\n",
    "\n",
    "        # # Path to the output text file\n",
    "        output_file_path = \"output.txt\"\n",
    "\n",
    "        # # Append the extracted portion to the text file as a new line\n",
    "        with open(output_file_path, 'a') as f:\n",
    "             f.write(desired_portion + '\\n')\n",
    "\n",
    "        print(f\"Appended '{desired_portion}' to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_91017/3994383485.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "The loss 0.4326261878013611\n",
      "The loss 0.08643348515033722\n",
      "The loss 0.2720319628715515\n",
      "The loss 0.18335837125778198\n",
      "The loss 1.0465641021728516\n",
      "The loss 0.6543803215026855\n",
      "The loss 0.3780989944934845\n",
      "The loss 0.707392692565918\n",
      "The loss 1.0432274341583252\n",
      "The loss 2.1238856315612793\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "from syncnet_dataset import Dataset\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank update\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=['self_attn.out_proj'],  # Targeting the attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer encoder\n",
    "model.transformer_encoder = get_peft_model(model.transformer_encoder, lora_config)\n",
    "\n",
    "# Define optimizer for fine-tuning\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', 'training_data/')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=10, shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for step, (x, mel, y) in enumerate(train_data_loader):\n",
    "        # Your training loop here\n",
    "        optimizer.zero_grad()\n",
    "        output, audio_embedding, face_embedding = model(x, mel)\n",
    "        loss = cross_entropy_loss(output, y) #if (global_epoch // 50) % 2 == 0 else contrastive_loss2(a, v, y)\n",
    "        print('The loss', loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), \"lora_adapter_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_91017/3994383485.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_91017/3367997342.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_weights = torch.load(\"lora_adapter_weights.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias'], unexpected_keys=['transformer_encoder.base_model.model.layers.0.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.0.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.0.linear1.weight', 'transformer_encoder.base_model.model.layers.0.linear1.bias', 'transformer_encoder.base_model.model.layers.0.linear2.weight', 'transformer_encoder.base_model.model.layers.0.linear2.bias', 'transformer_encoder.base_model.model.layers.0.norm1.weight', 'transformer_encoder.base_model.model.layers.0.norm1.bias', 'transformer_encoder.base_model.model.layers.0.norm2.weight', 'transformer_encoder.base_model.model.layers.0.norm2.bias', 'transformer_encoder.base_model.model.layers.1.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.1.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.1.linear1.weight', 'transformer_encoder.base_model.model.layers.1.linear1.bias', 'transformer_encoder.base_model.model.layers.1.linear2.weight', 'transformer_encoder.base_model.model.layers.1.linear2.bias', 'transformer_encoder.base_model.model.layers.1.norm1.weight', 'transformer_encoder.base_model.model.layers.1.norm1.bias', 'transformer_encoder.base_model.model.layers.1.norm2.weight', 'transformer_encoder.base_model.model.layers.1.norm2.bias', 'transformer_encoder.base_model.model.layers.2.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.2.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.2.linear1.weight', 'transformer_encoder.base_model.model.layers.2.linear1.bias', 'transformer_encoder.base_model.model.layers.2.linear2.weight', 'transformer_encoder.base_model.model.layers.2.linear2.bias', 'transformer_encoder.base_model.model.layers.2.norm1.weight', 'transformer_encoder.base_model.model.layers.2.norm1.bias', 'transformer_encoder.base_model.model.layers.2.norm2.weight', 'transformer_encoder.base_model.model.layers.2.norm2.bias', 'transformer_encoder.base_model.model.layers.3.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.3.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.3.linear1.weight', 'transformer_encoder.base_model.model.layers.3.linear1.bias', 'transformer_encoder.base_model.model.layers.3.linear2.weight', 'transformer_encoder.base_model.model.layers.3.linear2.bias', 'transformer_encoder.base_model.model.layers.3.norm1.weight', 'transformer_encoder.base_model.model.layers.3.norm1.bias', 'transformer_encoder.base_model.model.layers.3.norm2.weight', 'transformer_encoder.base_model.model.layers.3.norm2.bias', 'transformer_encoder.base_model.model.layers.4.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.4.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.4.linear1.weight', 'transformer_encoder.base_model.model.layers.4.linear1.bias', 'transformer_encoder.base_model.model.layers.4.linear2.weight', 'transformer_encoder.base_model.model.layers.4.linear2.bias', 'transformer_encoder.base_model.model.layers.4.norm1.weight', 'transformer_encoder.base_model.model.layers.4.norm1.bias', 'transformer_encoder.base_model.model.layers.4.norm2.weight', 'transformer_encoder.base_model.model.layers.4.norm2.bias', 'transformer_encoder.base_model.model.layers.5.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.5.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.5.linear1.weight', 'transformer_encoder.base_model.model.layers.5.linear1.bias', 'transformer_encoder.base_model.model.layers.5.linear2.weight', 'transformer_encoder.base_model.model.layers.5.linear2.bias', 'transformer_encoder.base_model.model.layers.5.norm1.weight', 'transformer_encoder.base_model.model.layers.5.norm1.bias', 'transformer_encoder.base_model.model.layers.5.norm2.weight', 'transformer_encoder.base_model.model.layers.5.norm2.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the base model\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Load the LoRA adapter weights\n",
    "lora_weights = torch.load(\"lora_adapter_weights.pth\")\n",
    "\n",
    "# Update only the LoRA weights (specific keys in the state dict)\n",
    "model.load_state_dict(lora_weights, strict=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

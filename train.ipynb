{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchvision librosa wandb matplotlib lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/spawn.py\", line 131, in _main\n",
      "    prepare(preparation_data)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/spawn.py\", line 246, in prepare\n",
      "    _fixup_main_from_path(data['init_main_from_path'])\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/spawn.py\", line 297, in _fixup_main_from_path\n",
      "    main_content = runpy.run_path(main_path,\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"<frozen runpy>\", line 286, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 20, in <module>\n",
      "    from syncnet_dataset import Dataset\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/syncnet_dataset.py\", line 12, in <module>\n",
      "    face_image_cache =  multiprocessing.Manager().dict()\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/context.py\", line 57, in Manager\n",
      "    m.start()\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/managers.py\", line 562, in start\n",
      "    self._process.start()\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/process.py\", line 121, in start\n",
      "    self._popen = self._Popen(self)\n",
      "                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/context.py\", line 289, in _Popen\n",
      "    return Popen(process_obj)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/popen_spawn_posix.py\", line 32, in __init__\n",
      "    super().__init__(process_obj)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/popen_spawn_posix.py\", line 42, in _launch\n",
      "    prep_data = spawn.get_preparation_data(process_obj._name)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/spawn.py\", line 164, in get_preparation_data\n",
      "    _check_not_importing_main()\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/spawn.py\", line 140, in _check_not_importing_main\n",
      "    raise RuntimeError('''\n",
      "RuntimeError: \n",
      "        An attempt has been made to start a new process before the\n",
      "        current process has finished its bootstrapping phase.\n",
      "\n",
      "        This probably means that you are not using fork to start your\n",
      "        child processes and you have forgotten to use the proper idiom\n",
      "        in the main module:\n",
      "\n",
      "            if __name__ == '__main__':\n",
      "                freeze_support()\n",
      "                ...\n",
      "\n",
      "        The \"freeze_support()\" line can be omitted if the program\n",
      "        is not going to be frozen to produce an executable.\n",
      "\n",
      "        To fix this issue, refer to the \"Safe importing of main module\"\n",
      "        section in https://docs.python.org/3/library/multiprocessing.html\n",
      "        \n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_syncnet_train.py\", line 20, in <module>\n",
      "    from syncnet_dataset import Dataset\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/syncnet_dataset.py\", line 12, in <module>\n",
      "    face_image_cache =  multiprocessing.Manager().dict()\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/context.py\", line 57, in Manager\n",
      "    m.start()\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/managers.py\", line 566, in start\n",
      "    self._address = reader.recv()\n",
      "                    ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/connection.py\", line 430, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "          ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/multiprocessing/connection.py\", line 399, in _recv\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "# syncnet training resume\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth \\\n",
    "--use_wandb False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 48192739\n",
      "Load checkpoint from: checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py:334: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "The learning rate is: 0.0001\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loading model from: /opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
      "0it [00:00, ?it/s]Saved checkpoint: checkpoints/checkpoint_step000000001.pth\n",
      "Step: 5, Img Loss: 0.17881047129631042, Sync Loss: 0.0, L1: 0.1548808306455612, L2: 0.038711801171302795, Disc: 0.6334737300872803, LR: 0.0001: : 5it [00:16,  3.33s/it]\n",
      "Step: 10, Img Loss: 0.15472855865955354, Sync Loss: 0.0, L1: 0.13181405067443847, L2: 0.027725791931152342, Disc: 0.5901043057441712, LR: 0.0001: : 5it [00:15,  3.08s/it]\n",
      "Step: 15, Img Loss: 0.14621567875146865, Sync Loss: 0.0, L1: 0.12539926171302795, L2: 0.02469371035695076, Disc: 0.5417276740074157, LR: 0.0001: : 5it [00:15,  3.12s/it] \n",
      "Step: 20, Img Loss: 0.12384374290704728, Sync Loss: 0.0, L1: 0.10381756722927094, L2: 0.017749984003603458, Disc: 0.5043411254882812, LR: 0.0001: : 5it [00:15,  3.11s/it]\n",
      "Step: 25, Img Loss: 0.11091931462287903, Sync Loss: 0.0, L1: 0.09323196560144424, L2: 0.015315253846347332, Disc: 0.4469789505004883, LR: 0.0001: : 5it [00:15,  3.08s/it]\n",
      "Step: 30, Img Loss: 0.12118886411190033, Sync Loss: 0.0, L1: 0.10412423312664032, L2: 0.017801598645746708, Disc: 0.44541686177253725, LR: 0.0001: : 5it [00:15,  3.09s/it]\n",
      "Step: 35, Img Loss: 0.10731754899024963, Sync Loss: 0.0, L1: 0.08955496326088905, L2: 0.014095530565828084, Disc: 0.444806706905365, LR: 0.0001: : 5it [00:15,  3.11s/it]  \n",
      "Step: 40, Img Loss: 0.11387632191181182, Sync Loss: 0.0, L1: 0.09783518239855767, L2: 0.016555131413042545, Disc: 0.4186579823493958, LR: 0.0001: : 5it [00:15,  3.17s/it]\n",
      "Step: 45, Img Loss: 0.0940866157412529, Sync Loss: 0.0, L1: 0.07880488187074661, L2: 0.011001799162477255, Disc: 0.38443958163261416, LR: 0.0001: : 5it [00:15,  3.16s/it] \n",
      "Step: 50, Img Loss: 0.09964188486337662, Sync Loss: 0.0, L1: 0.08649140521883965, L2: 0.012409445457160473, Disc: 0.3495010554790497, LR: 0.0001: : 5it [00:15,  3.11s/it] \n",
      "Step: 55, Img Loss: 0.09236134737730026, Sync Loss: 0.0, L1: 0.07844686880707741, L2: 0.011530206445604563, Disc: 0.356736421585083, LR: 0.0001: : 5it [00:15,  3.16s/it]  \n",
      "Step: 60, Img Loss: 0.09293688237667083, Sync Loss: 0.0, L1: 0.07915234044194222, L2: 0.011052432283759118, Disc: 0.35484314560890196, LR: 0.0001: : 5it [00:15,  3.06s/it]\n",
      "Step: 65, Img Loss: 0.09104801416397094, Sync Loss: 0.0, L1: 0.0779282197356224, L2: 0.010419359058141708, Disc: 0.3403241693973541, LR: 0.0001: : 5it [00:15,  3.16s/it] \n",
      "Step: 70, Img Loss: 0.09234860688447952, Sync Loss: 0.0, L1: 0.08014514222741127, L2: 0.011117730289697647, Disc: 0.32421447038650514, LR: 0.0001: : 5it [00:15,  3.11s/it]\n",
      "Step: 75, Img Loss: 0.09235714673995972, Sync Loss: 0.0, L1: 0.07910163030028343, L2: 0.01140676587820053, Disc: 0.34421196579933167, LR: 0.0001: : 5it [00:15,  3.10s/it] \n",
      "Step: 80, Img Loss: 0.07752740383148193, Sync Loss: 0.0, L1: 0.06509914845228196, L2: 0.007786712143570185, Disc: 0.31366424560546874, LR: 0.0001: : 5it [00:15,  3.12s/it]\n",
      "Step: 85, Img Loss: 0.10277209877967834, Sync Loss: 0.0, L1: 0.09140048176050186, L2: 0.014100608695298434, Disc: 0.3188328742980957, LR: 0.0001: : 5it [00:15,  3.09s/it] \n",
      "Step: 90, Img Loss: 0.10091981440782546, Sync Loss: 0.0, L1: 0.08945515751838684, L2: 0.01234233658760786, Disc: 0.3187482953071594, LR: 0.0001: : 5it [00:15,  3.12s/it]  \n",
      "Step: 95, Img Loss: 0.07893349677324295, Sync Loss: 0.0, L1: 0.06626916751265526, L2: 0.008743393607437611, Disc: 0.3195556879043579, LR: 0.0001: : 5it [00:15,  3.11s/it] \n",
      "Step: 99, Img Loss: 0.07017584145069122, Sync Loss: 0.0, L1: 0.0586426155641675, L2: 0.00707986083580181, Disc: 0.2893071621656418, LR: 0.0001: : 4it [00:12,  3.10s/it]   Saved checkpoint: checkpoints/checkpoint_step000000100.pth\n",
      "Step: 100, Img Loss: 0.07736526429653168, Sync Loss: 0.0, L1: 0.06583928242325783, L2: 0.00861436533741653, Disc: 0.29635895490646363, LR: 0.0001: : 5it [00:15,  3.19s/it]\n",
      "Step: 105, Img Loss: 0.07440569996833801, Sync Loss: 0.0, L1: 0.06218804940581322, L2: 0.007173453085124492, Disc: 0.3065410852432251, LR: 0.0001: : 5it [00:16,  3.23s/it] \n",
      "Step: 110, Img Loss: 0.07227874249219894, Sync Loss: 0.0, L1: 0.06099665686488152, L2: 0.007130600232630968, Disc: 0.28663836121559144, LR: 0.0001: : 5it [00:15,  3.16s/it]\n",
      "Step: 115, Img Loss: 0.08074544966220856, Sync Loss: 0.0, L1: 0.06963292360305787, L2: 0.00885979738086462, Disc: 0.29188345074653627, LR: 0.0001: : 5it [00:15,  3.11s/it] \n",
      "Step: 120, Img Loss: 0.08930454701185227, Sync Loss: 0.0, L1: 0.07856429889798164, L2: 0.010888661444187164, Disc: 0.2933692842721939, LR: 0.0001: : 5it [00:15,  3.08s/it] \n",
      "Step: 125, Img Loss: 0.08695029765367508, Sync Loss: 0.0, L1: 0.07715033292770386, L2: 0.009905507415533065, Disc: 0.27314963936805725, LR: 0.0001: : 5it [00:15,  3.10s/it] \n",
      "Step: 130, Img Loss: 0.07720274478197098, Sync Loss: 0.0, L1: 0.0656102791428566, L2: 0.007924037612974643, Disc: 0.2974595785140991, LR: 0.0001: : 5it [00:15,  3.11s/it]  \n",
      "Step: 135, Img Loss: 0.085557621717453, Sync Loss: 0.0, L1: 0.07474499791860581, L2: 0.009989104513078929, Disc: 0.29099748432636263, LR: 0.0001: : 5it [00:15,  3.15s/it]  \n",
      "Step: 140, Img Loss: 0.06837669163942336, Sync Loss: 0.0, L1: 0.0574009545147419, L2: 0.006321791745722294, Disc: 0.2769156754016876, LR: 0.0001: : 5it [00:15,  3.08s/it]  \n",
      "Step: 145, Img Loss: 0.08385894894599914, Sync Loss: 0.0, L1: 0.07415657937526703, L2: 0.009875734522938728, Disc: 0.26820400059223176, LR: 0.0001: : 5it [00:15,  3.07s/it]\n",
      "0it [00:00, ?it/s]^C\n"
     ]
    }
   ],
   "source": [
    "# Train wav2lip\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda: False\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "total trainable params 48192739\n",
      "Load checkpoint from: checkpoints/checkpoint_step000000100.pth\n",
      "/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py:335: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "Load optimizer state from checkpoints/checkpoint_step000000100.pth\n",
      "Load checkpoint from: checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\n",
      "The learning rate is: 1e-05\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Loading model from: /opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/lpips/lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
      "Step: 105, Img Loss: 0.08341320157051087, Sync Loss: 0.0, L1: 0.08341320157051087, L2: 0.013343592546880246, Disc: 0.0, LR: 1e-05: : 5it [00:10,  2.13s/it]\n",
      "Step: 110, Img Loss: 0.07131197899580002, Sync Loss: 0.0, L1: 0.07131197899580002, L2: 0.010619149543344975, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it]\n",
      "Step: 115, Img Loss: 0.07703548520803452, Sync Loss: 0.0, L1: 0.07703548520803452, L2: 0.01117193540558219, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.92s/it] \n",
      "Step: 120, Img Loss: 0.0799816906452179, Sync Loss: 0.0, L1: 0.0799816906452179, L2: 0.012805103603750468, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it]  \n",
      "Step: 125, Img Loss: 0.0703540325164795, Sync Loss: 0.0, L1: 0.0703540325164795, L2: 0.009875556360930205, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.95s/it]  \n",
      "Step: 130, Img Loss: 0.0764082558453083, Sync Loss: 0.0, L1: 0.0764082558453083, L2: 0.01072221975773573, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.92s/it]   \n",
      "Step: 135, Img Loss: 0.06960655376315117, Sync Loss: 0.0, L1: 0.06960655376315117, L2: 0.009636886417865753, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.90s/it]\n",
      "Step: 140, Img Loss: 0.08489115163683891, Sync Loss: 0.0, L1: 0.08489115163683891, L2: 0.013648082967847586, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.95s/it]\n",
      "Step: 145, Img Loss: 0.06491744965314865, Sync Loss: 0.0, L1: 0.06491744965314865, L2: 0.00853787288069725, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.91s/it]   \n",
      "Step: 150, Img Loss: 0.07704380303621292, Sync Loss: 0.0, L1: 0.07704380303621292, L2: 0.01062062093988061, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.94s/it] \n",
      "Step: 155, Img Loss: 0.06648966670036316, Sync Loss: 0.0, L1: 0.06648966670036316, L2: 0.008663030993193387, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.96s/it]\n",
      "Step: 160, Img Loss: 0.0698839396238327, Sync Loss: 0.0, L1: 0.0698839396238327, L2: 0.008964665327221155, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.87s/it]    \n",
      "Step: 165, Img Loss: 0.0717913530766964, Sync Loss: 0.0, L1: 0.0717913530766964, L2: 0.009480392280966043, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.96s/it]  \n",
      "Step: 170, Img Loss: 0.07360081970691681, Sync Loss: 0.0, L1: 0.07360081970691681, L2: 0.009560749866068363, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.89s/it]\n",
      "Step: 175, Img Loss: 0.07349093779921531, Sync Loss: 0.0, L1: 0.07349093779921531, L2: 0.009769312106072903, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.87s/it]\n",
      "Step: 180, Img Loss: 0.06685001924633979, Sync Loss: 0.0, L1: 0.06685001924633979, L2: 0.00837536109611392, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.91s/it]  \n",
      "Step: 185, Img Loss: 0.06213058531284332, Sync Loss: 0.0, L1: 0.06213058531284332, L2: 0.007941350247710944, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.87s/it]\n",
      "Step: 190, Img Loss: 0.08856700509786605, Sync Loss: 0.0, L1: 0.08856700509786605, L2: 0.014274006802588701, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.97s/it]\n",
      "Step: 195, Img Loss: 0.061322373896837236, Sync Loss: 0.0, L1: 0.061322373896837236, L2: 0.006853930745273828, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.89s/it]\n",
      "Step: 199, Img Loss: 0.06504403986036777, Sync Loss: 0.0, L1: 0.06504403986036777, L2: 0.008150048786774278, Disc: 0.0, LR: 1e-05: : 4it [00:07,  1.86s/it]Saved checkpoint: checkpoints/checkpoint_step000000200.pth\n",
      "Step: 200, Img Loss: 0.06651717871427536, Sync Loss: 0.0, L1: 0.06651717871427536, L2: 0.008398757502436637, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.96s/it]\n",
      "Step: 205, Img Loss: 0.08193348422646522, Sync Loss: 0.0, L1: 0.08193348422646522, L2: 0.011976479832082988, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.94s/it]\n",
      "Step: 210, Img Loss: 0.05792878568172455, Sync Loss: 0.0, L1: 0.05792878568172455, L2: 0.00656677563674748, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it] \n",
      "Step: 215, Img Loss: 0.06478309109807015, Sync Loss: 0.0, L1: 0.06478309109807015, L2: 0.007609062502160669, Disc: 0.0, LR: 1e-05: : 5it [00:10,  2.15s/it]\n",
      "Step: 220, Img Loss: 0.06379569321870804, Sync Loss: 0.0, L1: 0.06379569321870804, L2: 0.007933797594159842, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.92s/it]\n",
      "Step: 225, Img Loss: 0.07187088504433632, Sync Loss: 0.0, L1: 0.07187088504433632, L2: 0.00980854732915759, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.94s/it]   \n",
      "Step: 230, Img Loss: 0.06970012933015823, Sync Loss: 0.0, L1: 0.06970012933015823, L2: 0.008919975766912103, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it]\n",
      "Step: 235, Img Loss: 0.06445192471146584, Sync Loss: 0.0, L1: 0.06445192471146584, L2: 0.007945594564080238, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.92s/it]\n",
      "Step: 240, Img Loss: 0.06387476921081543, Sync Loss: 0.0, L1: 0.06387476921081543, L2: 0.007698737503960729, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.97s/it]\n",
      "Step: 245, Img Loss: 0.07589838653802872, Sync Loss: 0.0, L1: 0.07589838653802872, L2: 0.010317584499716759, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.97s/it]  \n",
      "Step: 250, Img Loss: 0.06590473204851151, Sync Loss: 0.0, L1: 0.06590473204851151, L2: 0.008485677745193243, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.91s/it]\n",
      "Step: 255, Img Loss: 0.07093856856226921, Sync Loss: 0.0, L1: 0.07093856856226921, L2: 0.00912482300773263, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.90s/it] \n",
      "Step: 260, Img Loss: 0.07403755933046341, Sync Loss: 0.0, L1: 0.07403755933046341, L2: 0.008997127413749695, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.91s/it]\n",
      "Step: 265, Img Loss: 0.06936384811997413, Sync Loss: 0.0, L1: 0.06936384811997413, L2: 0.009220694191753864, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.95s/it]  \n",
      "Step: 270, Img Loss: 0.06382695063948632, Sync Loss: 0.0, L1: 0.06382695063948632, L2: 0.008075114525854587, Disc: 0.0, LR: 1e-05: : 5it [00:09,  2.00s/it]\n",
      "Step: 275, Img Loss: 0.07987984567880631, Sync Loss: 0.0, L1: 0.07987984567880631, L2: 0.010467289295047522, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.96s/it]\n",
      "Step: 280, Img Loss: 0.0670758381485939, Sync Loss: 0.0, L1: 0.0670758381485939, L2: 0.007966138189658523, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.96s/it]    \n",
      "Step: 285, Img Loss: 0.0778158150613308, Sync Loss: 0.0, L1: 0.0778158150613308, L2: 0.010780366975814104, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it]  \n",
      "Step: 290, Img Loss: 0.06554951444268227, Sync Loss: 0.0, L1: 0.06554951444268227, L2: 0.008673832472413778, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.95s/it] \n",
      "Step: 295, Img Loss: 0.06063955053687096, Sync Loss: 0.0, L1: 0.06063955053687096, L2: 0.00698649138212204, Disc: 0.0, LR: 1e-05: : 5it [00:10,  2.00s/it]   \n",
      "Step: 299, Img Loss: 0.07410021312534809, Sync Loss: 0.0, L1: 0.07410021312534809, L2: 0.00976875820197165, Disc: 0.0, LR: 1e-05: : 4it [00:08,  2.00s/it] Saved checkpoint: checkpoints/checkpoint_step000000300.pth\n",
      "Step: 300, Img Loss: 0.06927671656012535, Sync Loss: 0.0, L1: 0.06927671656012535, L2: 0.008938646502792835, Disc: 0.0, LR: 1e-05: : 5it [00:10,  2.14s/it]\n",
      "Step: 305, Img Loss: 0.07836106941103935, Sync Loss: 0.0, L1: 0.07836106941103935, L2: 0.010742044914513827, Disc: 0.0, LR: 1e-05: : 5it [00:10,  2.05s/it]\n",
      "Step: 310, Img Loss: 0.06358356475830078, Sync Loss: 0.0, L1: 0.06358356475830078, L2: 0.0076282073743641375, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it]\n",
      "Step: 315, Img Loss: 0.07080308198928834, Sync Loss: 0.0, L1: 0.07080308198928834, L2: 0.008744035661220551, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.98s/it]\n",
      "Step: 320, Img Loss: 0.07397502064704894, Sync Loss: 0.0, L1: 0.07397502064704894, L2: 0.009667119476944208, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.94s/it]\n",
      "Step: 325, Img Loss: 0.0685251735150814, Sync Loss: 0.0, L1: 0.0685251735150814, L2: 0.008128789253532887, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.93s/it]  \n",
      "Step: 330, Img Loss: 0.07871171534061432, Sync Loss: 0.0, L1: 0.07871171534061432, L2: 0.010403222311288119, Disc: 0.0, LR: 1e-05: : 5it [00:09,  1.92s/it]\n",
      "Step: 331, Img Loss: 0.1119292676448822, Sync Loss: 0.0, L1: 0.1119292676448822, L2: 0.018843159079551697, Disc: 0.0, LR: 1e-05: : 1it [00:01,  2.00s/it]^C\n",
      "Step: 331, Img Loss: 0.1119292676448822, Sync Loss: 0.0, L1: 0.1119292676448822, L2: 0.018843159079551697, Disc: 0.0, LR: 1e-05: : 1it [00:02,  2.11s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py\", line 414, in <module>\n",
      "    train(device, model, train_data_loader, test_data_loader, optimizer,\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/transformer_wav2lip_train.py\", line 175, in train\n",
      "    g = model(indiv_mels, x)\n",
      "        ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/models/wav2lip.py\", line 126, in forward\n",
      "    this_face_sequence = f(this_face_sequence)\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/eddyma/DEV/Github/Wav2Lip/models/conv.py\", line 16, in forward\n",
      "    out = self.conv_block(x)\n",
      "          ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 219, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py\", line 176, in forward\n",
      "    return F.batch_norm(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/for_wav2lip/lib/python3.12/site-packages/torch/nn/functional.py\", line 2512, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# Train wav2lip resume\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth \\\n",
    "--checkpoint_path checkpoints/checkpoint_step000000300.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training file\n",
    "import os\n",
    "\n",
    "def get_subfolders(directory):\n",
    "    return [os.path.join(directory, name) for name in os.listdir(directory)\n",
    "            if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "for subdir in get_subfolders('training_data'):\n",
    "  for subdir2 in get_subfolders(subdir):\n",
    "    for root, dirs, files in os.walk(subdir2):\n",
    "        print(subdir2)\n",
    "        # Extract the desired portion (last two parts of the path)\n",
    "        desired_portion = os.path.join(*subdir2.split(os.sep)[-2:])\n",
    "\n",
    "        # # Path to the output text file\n",
    "        output_file_path = \"output.txt\"\n",
    "\n",
    "        # # Append the extracted portion to the text file as a new line\n",
    "        with open(output_file_path, 'a') as f:\n",
    "             f.write(desired_portion + '\\n')\n",
    "\n",
    "        print(f\"Appended '{desired_portion}' to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_91017/3994383485.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "The loss 0.4326261878013611\n",
      "The loss 0.08643348515033722\n",
      "The loss 0.2720319628715515\n",
      "The loss 0.18335837125778198\n",
      "The loss 1.0465641021728516\n",
      "The loss 0.6543803215026855\n",
      "The loss 0.3780989944934845\n",
      "The loss 0.707392692565918\n",
      "The loss 1.0432274341583252\n",
      "The loss 2.1238856315612793\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "from syncnet_dataset import Dataset\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank update\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=['self_attn.out_proj'],  # Targeting the attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer encoder\n",
    "model.transformer_encoder = get_peft_model(model.transformer_encoder, lora_config)\n",
    "\n",
    "# Define optimizer for fine-tuning\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', 'training_data/')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=10, shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for step, (x, mel, y) in enumerate(train_data_loader):\n",
    "        # Your training loop here\n",
    "        optimizer.zero_grad()\n",
    "        output, audio_embedding, face_embedding = model(x, mel)\n",
    "        loss = cross_entropy_loss(output, y) #if (global_epoch // 50) % 2 == 0 else contrastive_loss2(a, v, y)\n",
    "        print('The loss', loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), \"lora_adapter_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load checkpoint from: checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_91017/3994383485.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path,\n",
      "/var/folders/26/8h_tvw9n0j39wrmgcks9ky7c0000gn/T/ipykernel_91017/3367997342.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  lora_weights = torch.load(\"lora_adapter_weights.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['transformer_encoder.layers.0.self_attn.in_proj_weight', 'transformer_encoder.layers.0.self_attn.in_proj_bias', 'transformer_encoder.layers.0.self_attn.out_proj.weight', 'transformer_encoder.layers.0.self_attn.out_proj.bias', 'transformer_encoder.layers.0.linear1.weight', 'transformer_encoder.layers.0.linear1.bias', 'transformer_encoder.layers.0.linear2.weight', 'transformer_encoder.layers.0.linear2.bias', 'transformer_encoder.layers.0.norm1.weight', 'transformer_encoder.layers.0.norm1.bias', 'transformer_encoder.layers.0.norm2.weight', 'transformer_encoder.layers.0.norm2.bias', 'transformer_encoder.layers.1.self_attn.in_proj_weight', 'transformer_encoder.layers.1.self_attn.in_proj_bias', 'transformer_encoder.layers.1.self_attn.out_proj.weight', 'transformer_encoder.layers.1.self_attn.out_proj.bias', 'transformer_encoder.layers.1.linear1.weight', 'transformer_encoder.layers.1.linear1.bias', 'transformer_encoder.layers.1.linear2.weight', 'transformer_encoder.layers.1.linear2.bias', 'transformer_encoder.layers.1.norm1.weight', 'transformer_encoder.layers.1.norm1.bias', 'transformer_encoder.layers.1.norm2.weight', 'transformer_encoder.layers.1.norm2.bias', 'transformer_encoder.layers.2.self_attn.in_proj_weight', 'transformer_encoder.layers.2.self_attn.in_proj_bias', 'transformer_encoder.layers.2.self_attn.out_proj.weight', 'transformer_encoder.layers.2.self_attn.out_proj.bias', 'transformer_encoder.layers.2.linear1.weight', 'transformer_encoder.layers.2.linear1.bias', 'transformer_encoder.layers.2.linear2.weight', 'transformer_encoder.layers.2.linear2.bias', 'transformer_encoder.layers.2.norm1.weight', 'transformer_encoder.layers.2.norm1.bias', 'transformer_encoder.layers.2.norm2.weight', 'transformer_encoder.layers.2.norm2.bias', 'transformer_encoder.layers.3.self_attn.in_proj_weight', 'transformer_encoder.layers.3.self_attn.in_proj_bias', 'transformer_encoder.layers.3.self_attn.out_proj.weight', 'transformer_encoder.layers.3.self_attn.out_proj.bias', 'transformer_encoder.layers.3.linear1.weight', 'transformer_encoder.layers.3.linear1.bias', 'transformer_encoder.layers.3.linear2.weight', 'transformer_encoder.layers.3.linear2.bias', 'transformer_encoder.layers.3.norm1.weight', 'transformer_encoder.layers.3.norm1.bias', 'transformer_encoder.layers.3.norm2.weight', 'transformer_encoder.layers.3.norm2.bias', 'transformer_encoder.layers.4.self_attn.in_proj_weight', 'transformer_encoder.layers.4.self_attn.in_proj_bias', 'transformer_encoder.layers.4.self_attn.out_proj.weight', 'transformer_encoder.layers.4.self_attn.out_proj.bias', 'transformer_encoder.layers.4.linear1.weight', 'transformer_encoder.layers.4.linear1.bias', 'transformer_encoder.layers.4.linear2.weight', 'transformer_encoder.layers.4.linear2.bias', 'transformer_encoder.layers.4.norm1.weight', 'transformer_encoder.layers.4.norm1.bias', 'transformer_encoder.layers.4.norm2.weight', 'transformer_encoder.layers.4.norm2.bias', 'transformer_encoder.layers.5.self_attn.in_proj_weight', 'transformer_encoder.layers.5.self_attn.in_proj_bias', 'transformer_encoder.layers.5.self_attn.out_proj.weight', 'transformer_encoder.layers.5.self_attn.out_proj.bias', 'transformer_encoder.layers.5.linear1.weight', 'transformer_encoder.layers.5.linear1.bias', 'transformer_encoder.layers.5.linear2.weight', 'transformer_encoder.layers.5.linear2.bias', 'transformer_encoder.layers.5.norm1.weight', 'transformer_encoder.layers.5.norm1.bias', 'transformer_encoder.layers.5.norm2.weight', 'transformer_encoder.layers.5.norm2.bias'], unexpected_keys=['transformer_encoder.base_model.model.layers.0.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.0.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.0.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.0.linear1.weight', 'transformer_encoder.base_model.model.layers.0.linear1.bias', 'transformer_encoder.base_model.model.layers.0.linear2.weight', 'transformer_encoder.base_model.model.layers.0.linear2.bias', 'transformer_encoder.base_model.model.layers.0.norm1.weight', 'transformer_encoder.base_model.model.layers.0.norm1.bias', 'transformer_encoder.base_model.model.layers.0.norm2.weight', 'transformer_encoder.base_model.model.layers.0.norm2.bias', 'transformer_encoder.base_model.model.layers.1.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.1.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.1.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.1.linear1.weight', 'transformer_encoder.base_model.model.layers.1.linear1.bias', 'transformer_encoder.base_model.model.layers.1.linear2.weight', 'transformer_encoder.base_model.model.layers.1.linear2.bias', 'transformer_encoder.base_model.model.layers.1.norm1.weight', 'transformer_encoder.base_model.model.layers.1.norm1.bias', 'transformer_encoder.base_model.model.layers.1.norm2.weight', 'transformer_encoder.base_model.model.layers.1.norm2.bias', 'transformer_encoder.base_model.model.layers.2.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.2.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.2.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.2.linear1.weight', 'transformer_encoder.base_model.model.layers.2.linear1.bias', 'transformer_encoder.base_model.model.layers.2.linear2.weight', 'transformer_encoder.base_model.model.layers.2.linear2.bias', 'transformer_encoder.base_model.model.layers.2.norm1.weight', 'transformer_encoder.base_model.model.layers.2.norm1.bias', 'transformer_encoder.base_model.model.layers.2.norm2.weight', 'transformer_encoder.base_model.model.layers.2.norm2.bias', 'transformer_encoder.base_model.model.layers.3.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.3.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.3.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.3.linear1.weight', 'transformer_encoder.base_model.model.layers.3.linear1.bias', 'transformer_encoder.base_model.model.layers.3.linear2.weight', 'transformer_encoder.base_model.model.layers.3.linear2.bias', 'transformer_encoder.base_model.model.layers.3.norm1.weight', 'transformer_encoder.base_model.model.layers.3.norm1.bias', 'transformer_encoder.base_model.model.layers.3.norm2.weight', 'transformer_encoder.base_model.model.layers.3.norm2.bias', 'transformer_encoder.base_model.model.layers.4.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.4.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.4.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.4.linear1.weight', 'transformer_encoder.base_model.model.layers.4.linear1.bias', 'transformer_encoder.base_model.model.layers.4.linear2.weight', 'transformer_encoder.base_model.model.layers.4.linear2.bias', 'transformer_encoder.base_model.model.layers.4.norm1.weight', 'transformer_encoder.base_model.model.layers.4.norm1.bias', 'transformer_encoder.base_model.model.layers.4.norm2.weight', 'transformer_encoder.base_model.model.layers.4.norm2.bias', 'transformer_encoder.base_model.model.layers.5.self_attn.in_proj_weight', 'transformer_encoder.base_model.model.layers.5.self_attn.in_proj_bias', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.base_layer.weight', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.base_layer.bias', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.lora_A.default.weight', 'transformer_encoder.base_model.model.layers.5.self_attn.out_proj.lora_B.default.weight', 'transformer_encoder.base_model.model.layers.5.linear1.weight', 'transformer_encoder.base_model.model.layers.5.linear1.bias', 'transformer_encoder.base_model.model.layers.5.linear2.weight', 'transformer_encoder.base_model.model.layers.5.linear2.bias', 'transformer_encoder.base_model.model.layers.5.norm1.weight', 'transformer_encoder.base_model.model.layers.5.norm1.bias', 'transformer_encoder.base_model.model.layers.5.norm2.weight', 'transformer_encoder.base_model.model.layers.5.norm2.bias'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the base model\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Load the LoRA adapter weights\n",
    "lora_weights = torch.load(\"lora_adapter_weights.pth\")\n",
    "\n",
    "# Update only the LoRA weights (specific keys in the state dict)\n",
    "model.load_state_dict(lora_weights, strict=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

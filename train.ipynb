{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchvision librosa wandb matplotlib lpips mediapipe pytorch-msssim scikit-image piq realesrgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -L -o checkpoints/RealESRGAN_x4plus.pth https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syncnet training resume\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000188400.pth \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/wav2lip_checkpoint/more-ref \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000202200.pth \\\n",
    "--train_root local_train_files \\\n",
    "--num_of_unet_layers 1 \\\n",
    "--use_augmentation False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip resume\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/wav2lip_checkpoint/face-enhancer \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000221800.pth \\\n",
    "--checkpoint_path checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000139000.pth \\\n",
    "--train_root local_train_files/ \\\n",
    "--use_augmentation False \\\n",
    "--num_of_unet_layers 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training file\n",
    "import os\n",
    "\n",
    "def get_subfolders(directory):\n",
    "    return [os.path.join(directory, name) for name in os.listdir(directory)\n",
    "            if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "for subdir in get_subfolders('training_data'):\n",
    "  for subdir2 in get_subfolders(subdir):\n",
    "    for root, dirs, files in os.walk(subdir2):\n",
    "        print(subdir2)\n",
    "        # Extract the desired portion (last two parts of the path)\n",
    "        desired_portion = os.path.join(*subdir2.split(os.sep)[-2:])\n",
    "\n",
    "        # # Path to the output text file\n",
    "        output_file_path = \"output.txt\"\n",
    "\n",
    "        # # Append the extracted portion to the text file as a new line\n",
    "        with open(output_file_path, 'a') as f:\n",
    "             f.write(desired_portion + '\\n')\n",
    "\n",
    "        print(f\"Appended '{desired_portion}' to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_wav2lip_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA fine tuning syncnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_syncnet_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "from syncnet_dataset import Dataset\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_syncnet_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank update\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=['self_attn.out_proj'],  # Targeting the attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer encoder\n",
    "model.transformer_encoder = get_peft_model(model.transformer_encoder, lora_config)\n",
    "\n",
    "# Define optimizer for fine-tuning\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', 'training_data/')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=10, shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for step, (x, mel, y) in enumerate(train_data_loader):\n",
    "        # Your training loop here\n",
    "        optimizer.zero_grad()\n",
    "        output, audio_embedding, face_embedding = model(x, mel)\n",
    "        loss = cross_entropy_loss(output, y) #if (global_epoch // 50) % 2 == 0 else contrastive_loss2(a, v, y)\n",
    "        print('The loss', loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), \"lora_adapter_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_syncnet_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Load the LoRA adapter weights\n",
    "lora_weights = torch.load(\"lora_syncnet_weights.pth\")\n",
    "\n",
    "# Update only the LoRA weights (specific keys in the state dict)\n",
    "model.load_state_dict(lora_weights, strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora fine tune wav2lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import LoRAConv2d, ResUNet, LoRATransposeConv2d  # Assume you have a LoRA-wrapped Conv2d\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "checkpoint_path = \"checkpoints/wav2lip_checkpoint/2-step-process/checkpoint_step000042000.pth\"\n",
    "lora_checkpoint_point = \"checkpoints/wav2lip_lora/lora_wav2lip_eddy_weights.pth\"\n",
    "\n",
    "def apply_lora_to_model(model, lora_rank=4, lora_scaling=0.5):\n",
    "    num_of_layers = 500\n",
    "    acc = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if acc >= num_of_layers:\n",
    "            break\n",
    "        \n",
    "        \n",
    "        \n",
    "        if ( \"output_block.output_block\" in name.lower()):\n",
    "            \n",
    "            if isinstance(module, nn.Conv2d):\n",
    "              print('The module name', name)\n",
    "              # Create a LoRA-wrapped Conv2d layer\n",
    "              lora_module = LoRAConv2d(\n",
    "                  in_channels=module.in_channels,\n",
    "                  out_channels=module.out_channels,\n",
    "                  kernel_size=module.kernel_size,\n",
    "                  stride=module.stride,\n",
    "                  padding=module.padding,\n",
    "                  dilation=module.dilation,\n",
    "                  groups=module.groups,\n",
    "                  bias=(module.bias is not None),\n",
    "                  lora_rank=lora_rank,\n",
    "                  lora_scaling=lora_scaling\n",
    "              )\n",
    "              # Copy the weights from the original module\n",
    "              lora_module.conv.weight = module.weight\n",
    "              if module.bias is not None:\n",
    "                  lora_module.conv.bias = module.bias\n",
    "              # Replace the module in the model\n",
    "              parent_module = model\n",
    "              *parent_names, child_name = name.split('.')\n",
    "              for parent_name in parent_names:\n",
    "                  parent_module = getattr(parent_module, parent_name)\n",
    "              setattr(parent_module, child_name, lora_module)\n",
    "\n",
    "              acc += 1\n",
    "\n",
    "            # if isinstance(module, nn.ConvTranspose2d):\n",
    "            #   print('The tranpose module name', name)\n",
    "            #   # Create a LoRA-wrapped Conv2d layer\n",
    "            #   lora_module = LoRATransposeConv2d(\n",
    "            #       in_channels=module.in_channels,\n",
    "            #       out_channels=module.out_channels,\n",
    "            #       kernel_size=module.kernel_size,\n",
    "            #       stride=module.stride,\n",
    "            #       padding=module.padding,\n",
    "            #       lora_rank=lora_rank,\n",
    "            #       lora_scaling=lora_scaling\n",
    "            #   )\n",
    "            #   # Copy the weights from the original module\n",
    "            #   lora_module.conv.weight = module.weight\n",
    "            #   if module.bias is not None:\n",
    "            #       lora_module.conv.bias = module.bias\n",
    "            #   # Replace the module in the model\n",
    "            #   parent_module = model\n",
    "            #   *parent_names, child_name = name.split('.')\n",
    "            #   for parent_name in parent_names:\n",
    "            #       parent_module = getattr(parent_module, parent_name)\n",
    "            #   setattr(parent_module, child_name, lora_module)\n",
    "\n",
    "            #   acc += 1\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    if use_cuda:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "    else:\n",
    "        checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "\n",
    "from torch import optim\n",
    "from wav2lip_dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data as data_utils\n",
    "\n",
    "\n",
    "\n",
    "recon_loss = nn.L1Loss()\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = Dataset('train', 'preprocessed', 'lora_training_files', False)\n",
    "train_data_loader = data_utils.DataLoader(train_dataset, batch_size=10, shuffle=True,num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Instantiate your model\n",
    "model = ResUNet()\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=0.00001)\n",
    "\n",
    "load_wav2lip_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = apply_lora_to_model(model, lora_rank=4, lora_scaling=1)\n",
    "\n",
    "#print(model)\n",
    "\n",
    "# Freeze original model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze LoRA parameters\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRAConv2d):\n",
    "        module.lora_A.requires_grad = True\n",
    "        module.lora_B.requires_grad = True\n",
    "\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "prog_bar = tqdm(enumerate(train_data_loader))\n",
    "nepochs = 100\n",
    "epoch = 1\n",
    "while epoch <= nepochs:\n",
    "  #for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "  for step, (x, indiv_mels, mel, gt) in enumerate(train_data_loader):\n",
    "    model.train()\n",
    "    # Your training loop here\n",
    "    optimizer.zero_grad()\n",
    "    g =  model(indiv_mels, x)\n",
    "    loss = recon_loss(g, gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Step: {}, Epoch: {}, Sync Loss: {}, L1: {}'.format(step, epoch, 0, loss.item()))\n",
    "\n",
    "  epoch += 1\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), lora_checkpoint_point)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "model = Wav2Lip()\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=0.001)\n",
    "\n",
    "load_wav2lip_checkpoint(checkpoint_path, model, optimizer, reset_optimizer=True)\n",
    "\n",
    "# Load the LoRA fine-tuned weights\n",
    "lora_params = torch.load(lora_checkpoint_point)\n",
    "\n",
    "# Load the LoRA weights into the original model without changing the original weights\n",
    "model.load_state_dict(lora_params, strict=False)\n",
    "\n",
    "val_dataset = Dataset('val', 'training_data')\n",
    "val_data_loader = data_utils.DataLoader(val_dataset, batch_size=1, shuffle=True,num_workers=0)\n",
    "\n",
    "for x, indiv_mels, mel, gt in val_data_loader:\n",
    "            if x.shape[0] == 1:\n",
    "              step += 1\n",
    "              model.eval()\n",
    "\n",
    "              # Move data to CUDA device\n",
    "              x = x.to(device)\n",
    "              gt = gt.to(device)\n",
    "              indiv_mels = indiv_mels.to(device)\n",
    "              mel = mel.to(device)\n",
    "\n",
    "              g = model(indiv_mels, x)\n",
    "              \n",
    "              l1loss = recon_loss(g, gt)\n",
    "              print('The eval L1 loss', l1loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python inference.py \\\n",
    "--checkpoint_path checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000505000_striped.pth \\\n",
    "--face input/eddy.mp4 \\\n",
    "--audio input/en_1.wav \\\n",
    "--outfile results/eddy_505000_1layers_gen_en1_1.mp4 \\\n",
    "--model_layers 3 \\\n",
    "--use_ref_img True \\\n",
    "--use_esrgan False \\\n",
    "--iteration 1 \n",
    "#--lora_checkpoint_path checkpoints/wav2lip_lora/lora_wav2lip_eddy_weights.pth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Facemesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_all_file_paths(base_dir):\n",
    "    file_paths = []  # List to store all file paths\n",
    "\n",
    "    # Walk through each directory and subdirectory\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        for file in files:\n",
    "            # Construct full file path\n",
    "            full_path = os.path.join(root, file)\n",
    "            # Append the full path to the list\n",
    "            file_paths.append(full_path)\n",
    "\n",
    "    return file_paths\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True, min_detection_confidence=0.5)\n",
    "\n",
    "# Define your base directory\n",
    "base_dir = 'training_data/'\n",
    "\n",
    "# Get all file paths\n",
    "all_file_paths = get_all_file_paths(base_dir)\n",
    "\n",
    "for fname in all_file_paths:\n",
    "  if fname.lower().endswith(\".jpg\") and not fname.lower().endswith(\"_landmarks.jpg\"):\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.resize(img, (192, 192))\n",
    "    result = face_mesh.process(img)\n",
    "        \n",
    "    # Initialize the landmark channel as a zero matrix\n",
    "    landmark_channel = np.zeros((192, 192), dtype=np.uint8)\n",
    "    output_fname = fname.replace('.jpg', '_landmarks.jpg')  # Modify this line based on your file naming preference\n",
    "\n",
    "    if result.multi_face_landmarks:\n",
    "            \n",
    "      for face_landmarks in result.multi_face_landmarks:\n",
    "        for landmark in face_landmarks.landmark:\n",
    "          # Convert landmark to pixel coordinates\n",
    "          \n",
    "          x = min(int(math.floor(landmark.x * 192)), 191)\n",
    "          y = min(int(math.floor(landmark.y * 192)), 191)\n",
    "\n",
    "          # Mark the corresponding location in the landmark_channel\n",
    "          landmark_channel[y, x] = 255  # Mark as white pixel for the landmark\n",
    "          # Save the landmark channel as an image\n",
    "          cv2.imwrite(output_fname, landmark_channel)\n",
    "          \n",
    "    else:\n",
    "      print('No face', fname)\n",
    "      cv2.imwrite(output_fname, landmark_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = 'training_data/'\n",
    "\n",
    "# Get all file paths\n",
    "all_file_paths = get_all_file_paths(base_dir)\n",
    "\n",
    "for fname in all_file_paths:\n",
    "    if fname.lower().endswith(\"_landmarks.jpg\"):\n",
    "        try:\n",
    "            os.remove(fname)\n",
    "            print(f\"Deleted: {fname}\")\n",
    "        except OSError as e:\n",
    "            print(f\"Error deleting {fname}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "face_data = torch.randn(5, 15, 96, 192) \n",
    "audio_data = torch.randn(5, 1, 80, 16)\n",
    "\n",
    "y = model(face_data, audio_data)\n",
    "#make_dot(y, params=dict(model.named_parameters())).render(\"model_architecture_param\", format=\"png\")\n",
    "\n",
    "make_dot(y).render(\"simplified_syncnet_model\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "from models import Wav2Lip\n",
    "\n",
    "model = Wav2Lip(num_of_blocks=2)\n",
    "\n",
    "face_data = torch.randn(5, 9, 192, 192)\n",
    "audio_data = torch.randn(5, 1, 80, 16)\n",
    "\n",
    "y = model(audio_data, face_data)\n",
    "#make_dot(y, params=dict(model.named_parameters())).render(\"model_architecture_param\", format=\"png\")\n",
    "\n",
    "make_dot(y).render(\"simplified_wav2lip_model\", format=\"png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def calculate_psnr(pred_img, target_img, max_pixel_value=255.0):\n",
    "    \"\"\"\n",
    "    Calculate the PSNR between two images.\n",
    "    \n",
    "    Args:\n",
    "        pred_img (torch.Tensor): Generated image tensor of shape (batch, channels, height, width)\n",
    "        target_img (torch.Tensor): Ground truth image tensor of shape (batch, channels, height, width)\n",
    "        max_pixel_value (float): Maximum pixel value (255 for 8-bit images)\n",
    "    \n",
    "    Returns:\n",
    "        psnr (float): Peak Signal-to-Noise Ratio value\n",
    "    \"\"\"\n",
    "    # Calculate the Mean Squared Error (MSE)\n",
    "    mse = F.mse_loss(pred_img, target_img)\n",
    "    \n",
    "    if mse == 0:\n",
    "        return float('inf')  # If MSE is zero, PSNR is infinite\n",
    "\n",
    "    psnr = 20 * torch.log10(torch.tensor(max_pixel_value)) - 10 * torch.log10(mse)\n",
    "    \n",
    "    return psnr.item()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from wav2lip_dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data as data_utils\n",
    "from models import ResUNet\n",
    "import lpips\n",
    "import torch\n",
    "import piq\n",
    "from pytorch_msssim import ms_ssim, ssim\n",
    "import cv2\n",
    "\n",
    "\n",
    "print(piq.__version__)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "dataset_path = 'metrics'\n",
    "training_files_path = 'metrics_files'\n",
    "\n",
    "# dataset_path = 'training_data'\n",
    "# training_files_path = 'local_train_files'\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', dataset_path, training_files_path, False)\n",
    "train_data_loader = data_utils.DataLoader(train_dataset, batch_size=4, shuffle=True,num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Instantiate your model\n",
    "#checkpoint_path = \"checkpoints/wav2lip_checkpoint/more-ref/checkpoint_step000116000.pth\"\n",
    "#checkpoint_path = \"checkpoints/wav2lip_checkpoint/more-ref-2layers/checkpoint_step000182000.pth\"\n",
    "\n",
    "#checkpoint_path = \"checkpoints/wav2lip_checkpoint/more-ref-3layers/checkpoint_step000505000.pth\"\n",
    "checkpoint_path = \"checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000160000.pth\"\n",
    "\n",
    "\n",
    "model = ResUNet(1)\n",
    "\n",
    "load_wav2lip_checkpoint(checkpoint_path, model, None, reset_optimizer=True)\n",
    "\n",
    "lpips_loss = lpips.LPIPS(net='vgg').to(device)  # You can choose 'alex', 'vgg', or 'squeeze'\n",
    "\n",
    "prog_bar = tqdm(enumerate(train_data_loader))\n",
    "\n",
    "iteration = 10\n",
    "total_psnr = 0\n",
    "total_lpips = 0\n",
    "total_ssim = 0\n",
    "total_brisque = 0\n",
    "\n",
    "global_loipses = []\n",
    "global_ssim = []\n",
    "global_brisque = []\n",
    "\n",
    "psnr_max = 0.0\n",
    "ssim_max = 0.0\n",
    "lipis_min = 10.0\n",
    "brisque_min = 100.0\n",
    "\n",
    "        \n",
    "\n",
    "#model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "  for x in range(iteration):\n",
    "    for step, (x, indiv_mels, mel, gt) in enumerate(train_data_loader):\n",
    "      \n",
    "      g =  model(indiv_mels, x)\n",
    "      g = torch.clamp(g, 0, 1)\n",
    "      \n",
    "      img = (g.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255).astype(np.uint8)  # Shape becomes [1, 5, 192, 192, 3]\n",
    "      gt2 = (gt.detach().cpu().numpy().transpose(0, 2, 3, 4, 1) * 255).astype(np.uint8)  # Shape becomes [1, 5, 192, 192, 3]\n",
    "\n",
    "      #print(\"img min:\", img.min().item(), \"img max:\", img.max().item())\n",
    "\n",
    "      # Step 2: Loop through batch and time dimensions to save each frame individually\n",
    "      for batch_idx, batch in enumerate(img):\n",
    "          for t, frame in enumerate(batch):\n",
    "              # Save each frame with unique filename\n",
    "              filename = f'checkpoints/wav2lip_checkpoint/face-enhancer/{batch_idx}_{t}.jpg'\n",
    "              cv2.imwrite(filename, frame)\n",
    "              #print(f\"Saved {filename}\")\n",
    "      \n",
    "      # Calculate PSNR\n",
    "      p = torch.tensor((g.detach().cpu().numpy() * 255).astype(np.uint8)).float()\n",
    "      t = torch.tensor((gt.detach().cpu().numpy() * 255).astype(np.uint8)).float()\n",
    "      psnr_value = calculate_psnr(p, t)\n",
    "      total_psnr += psnr_value\n",
    "\n",
    "      num_of_frames = g.shape[2]\n",
    "      disc_losses = []\n",
    "      ssim_losses = []\n",
    "      brisque_scores = []\n",
    "      \n",
    "\n",
    "      for i in range(num_of_frames):\n",
    "        gen_frame = g[:, :, i, :, :]  # Shape: [batch_size, 3, 192, 192]\n",
    "        gt_frame = gt[:, :, i, :, :]    # Shape: [batch_size, 3, 192, 192]\n",
    "        \n",
    "        lpips_f_loss = lpips_loss(gen_frame.to(device), gt_frame.to(device))\n",
    "        disc_losses.append(lpips_f_loss)\n",
    "\n",
    "        total_lpips += lpips_f_loss\n",
    "\n",
    "        # Calculate MS-SSIM\n",
    "        ms_ssim_value = ms_ssim(gen_frame, gt_frame, data_range=1.0)\n",
    "        ssim_losses.append(ms_ssim_value)\n",
    "        total_ssim += ms_ssim_value\n",
    "\n",
    "        score = piq.brisque(gen_frame)\n",
    "        brisque_scores.append(score)\n",
    "        total_brisque += score\n",
    "      \n",
    "      avg_lpips = torch.mean(torch.stack(disc_losses))\n",
    "      avg_ssim = torch.mean(torch.stack(ssim_losses))\n",
    "      avg_brisque_score = torch.mean(torch.stack(brisque_scores))\n",
    "      \n",
    "      global_loipses.append(avg_lpips)\n",
    "      global_ssim.append(avg_ssim)\n",
    "      global_brisque.append(avg_brisque_score)\n",
    "      \n",
    "      lpips_loss_value = torch.min(torch.stack(disc_losses))\n",
    "      ssim_value = torch.max(torch.stack(ssim_losses))\n",
    "      brisque_score = torch.min(torch.stack(brisque_scores))\n",
    "\n",
    "      if psnr_value > psnr_max:\n",
    "        psnr_max = psnr_value\n",
    "\n",
    "      if lpips_loss_value < lipis_min:\n",
    "        lipis_min = lpips_loss_value\n",
    "\n",
    "      if ssim_value > ssim_max:\n",
    "        ssim_max = ssim_value\n",
    "\n",
    "      if brisque_score < brisque_min:\n",
    "        brisque_min = brisque_score\n",
    "\n",
    "      print(f\"PSNR(higher the better): {psnr_value} dB, LIPIS(lower the better): {lpips_loss_value}, MS-SSIM(higher the better): {ssim_value}, BRISQUE(lower the better): {brisque_score}\")\n",
    "\n",
    "print(f\"The max PSNR: {psnr_max} dB, LIPIS: {lipis_min}, MS-SSIM: {ssim_max}, BRISQUE: {brisque_min}\")\n",
    "print(f\"The avg PSNR: {total_psnr/iteration} dB, LIPIS: {torch.min(torch.stack(global_loipses))}, MS-SSIM: {torch.min(torch.stack(global_ssim))}, BRISQUE: {torch.min(torch.stack(global_brisque))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Load the original model and its weights\n",
    "model = ResUNet(3)\n",
    "load_wav2lip_checkpoint('checkpoints/wav2lip_checkpoint/more-ref-3layers/checkpoint_step000505000.pth', model, None, reset_optimizer=True)\n",
    "\n",
    "# Load the fine-tuned model and its weights\n",
    "# fine_tuned_model = ResUNet(3)  # Instantiate the model class again if needed\n",
    "# load_wav2lip_checkpoint('checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000552500.pth', fine_tuned_model, None, reset_optimizer=True)\n",
    "\n",
    "# Transfer weights for frozen layers from the original model\n",
    "# for name, param in fine_tuned_model.named_parameters():\n",
    "#     if name in model.state_dict() and 'face_enhancer' in name:\n",
    "#         print('Copying', name)\n",
    "#         model.state_dict()[name].copy_(param)\n",
    "\n",
    "# Save the combined model\n",
    "torch.save({\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": None,\n",
    "        \"global_step\": 505000,\n",
    "        \"global_epoch\": 100,\n",
    "    }, 'checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000505000_striped.pth')\n",
    "\n",
    "#torch.save(fine_tuned_model.state_dict(), \"checkpoints/wav2lip_checkpoint/face-enhancer/checkpoint_step000505000_combined.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

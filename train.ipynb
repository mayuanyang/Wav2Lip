{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm torchvision librosa wandb matplotlib lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: transformer_syncnet_train.py [-h] --data_root DATA_ROOT\n",
      "                                    --checkpoint_dir CHECKPOINT_DIR\n",
      "                                    [--checkpoint_path CHECKPOINT_PATH]\n",
      "                                    [--train_root TRAIN_ROOT]\n",
      "                                    [--use_cosine_loss USE_COSINE_LOSS]\n",
      "                                    [--sample_mode SAMPLE_MODE]\n",
      "                                    [--use_wandb USE_WANDB]\n",
      "transformer_syncnet_train.py: error: unrecognized arguments: \n"
     ]
    }
   ],
   "source": [
    "# Train syncnet from scrach\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syncnet training resume\n",
    "!python transformer_syncnet_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints/syncnet_checkpoint \\\n",
    "--checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000124000.pth \\\n",
    "--use_wandb False \\\n",
    "--train_root local_train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train wav2lip resume\n",
    "!python transformer_wav2lip_train.py --data_root training_data/ \\\n",
    "--checkpoint_dir checkpoints \\\n",
    "--use_wandb False \\\n",
    "--syncnet_checkpoint_path checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth \\\n",
    "--checkpoint_path checkpoints/checkpoint_step000004600.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the training file\n",
    "import os\n",
    "\n",
    "def get_subfolders(directory):\n",
    "    return [os.path.join(directory, name) for name in os.listdir(directory)\n",
    "            if os.path.isdir(os.path.join(directory, name))]\n",
    "\n",
    "for subdir in get_subfolders('training_data'):\n",
    "  for subdir2 in get_subfolders(subdir):\n",
    "    for root, dirs, files in os.walk(subdir2):\n",
    "        print(subdir2)\n",
    "        # Extract the desired portion (last two parts of the path)\n",
    "        desired_portion = os.path.join(*subdir2.split(os.sep)[-2:])\n",
    "\n",
    "        # # Path to the output text file\n",
    "        output_file_path = \"output.txt\"\n",
    "\n",
    "        # # Append the extracted portion to the text file as a new line\n",
    "        with open(output_file_path, 'a') as f:\n",
    "             f.write(desired_portion + '\\n')\n",
    "\n",
    "        print(f\"Appended '{desired_portion}' to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA fine tuning syncnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install peft numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_syncnet_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model\n",
    "\n",
    "def _load(checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "    return checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils import data as data_utils\n",
    "from models import TransformerSyncnet as TransformerSyncnet\n",
    "from syncnet_dataset import Dataset\n",
    "\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_syncnet_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Define the LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank update\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=['self_attn.out_proj'],  # Targeting the attention layers\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "# Apply LoRA to the transformer encoder\n",
    "model.transformer_encoder = get_peft_model(model.transformer_encoder, lora_config)\n",
    "\n",
    "# Define optimizer for fine-tuning\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "train_dataset = Dataset('train', 'training_data/')\n",
    "\n",
    "train_data_loader = data_utils.DataLoader(\n",
    "        train_dataset, batch_size=10, shuffle=True,\n",
    "        num_workers=0)\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "for epoch in range(2):\n",
    "    model.train()\n",
    "    for step, (x, mel, y) in enumerate(train_data_loader):\n",
    "        # Your training loop here\n",
    "        optimizer.zero_grad()\n",
    "        output, audio_embedding, face_embedding = model(x, mel)\n",
    "        loss = cross_entropy_loss(output, y) #if (global_epoch // 50) % 2 == 0 else contrastive_loss2(a, v, y)\n",
    "        print('The loss', loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), \"lora_adapter_weights.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "# Step 1: Create an instance of the TransformerSyncnet model\n",
    "model = TransformerSyncnet(num_heads=8, num_encoder_layers=6)\n",
    "\n",
    "# Step 2: Load the pre-trained weights\n",
    "load_syncnet_checkpoint(\"checkpoints/syncnet_checkpoint/checkpoint_step000066000.pth\", model, None, reset_optimizer=True, overwrite_global_states=False)\n",
    "\n",
    "# Load the LoRA adapter weights\n",
    "lora_weights = torch.load(\"lora_syncnet_weights.pth\")\n",
    "\n",
    "# Update only the LoRA weights (specific keys in the state dict)\n",
    "model.load_state_dict(lora_weights, strict=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lora fine tune wav2lip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models import LoRAConv2d, Wav2Lip  # Assume you have a LoRA-wrapped Conv2d\n",
    "\n",
    "def apply_lora_to_model(model, lora_rank=4):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Create a LoRA-wrapped Conv2d layer\n",
    "            lora_module = LoRAConv2d(\n",
    "                in_channels=module.in_channels,\n",
    "                out_channels=module.out_channels,\n",
    "                kernel_size=module.kernel_size,\n",
    "                stride=module.stride,\n",
    "                padding=module.padding,\n",
    "                dilation=module.dilation,\n",
    "                groups=module.groups,\n",
    "                bias=(module.bias is not None),\n",
    "                lora_rank=lora_rank\n",
    "            )\n",
    "            # Copy the weights from the original module\n",
    "            lora_module.conv.weight = module.weight\n",
    "            if module.bias is not None:\n",
    "                lora_module.conv.bias = module.bias\n",
    "            # Replace the module in the model\n",
    "            parent_module = model\n",
    "            *parent_names, child_name = name.split('.')\n",
    "            for parent_name in parent_names:\n",
    "                parent_module = getattr(parent_module, parent_name)\n",
    "            setattr(parent_module, child_name, lora_module)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav2lip_checkpoint(path, model, optimizer, reset_optimizer=False, overwrite_global_states=True):\n",
    "    global global_step\n",
    "    global global_epoch\n",
    "\n",
    "    print(\"Load checkpoint from: {}\".format(path))\n",
    "    checkpoint = _load(path)\n",
    "    s = checkpoint[\"state_dict\"]\n",
    "    new_s = {}\n",
    "    for k, v in s.items():\n",
    "        new_s[k.replace('module.', '')] = v\n",
    "    model.load_state_dict(new_s, strict=False)\n",
    "    if not reset_optimizer:\n",
    "        optimizer_state = checkpoint[\"optimizer\"]\n",
    "        if optimizer_state is not None:\n",
    "            print(\"Load optimizer state from {}\".format(path))\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    if overwrite_global_states:\n",
    "        global_step = checkpoint[\"global_step\"]\n",
    "        global_epoch = checkpoint[\"global_epoch\"]\n",
    "\n",
    "    if optimizer != None:\n",
    "      for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = 0.00001\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from wav2lip_dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from torch.utils import data as data_utils\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "recon_loss = nn.L1Loss()\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataset = Dataset('train', 'training_data')\n",
    "train_data_loader = data_utils.DataLoader(train_dataset, batch_size=10, shuffle=True,num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# Instantiate your model\n",
    "model = Wav2Lip()\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=0.001)\n",
    "\n",
    "load_wav2lip_checkpoint(\"checkpoints/checkpoint_step000136000.pth\", model, optimizer, reset_optimizer=True)\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = apply_lora_to_model(model, lora_rank=2)\n",
    "\n",
    "#print(model)\n",
    "\n",
    "# Freeze original model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze LoRA parameters\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, LoRAConv2d):\n",
    "        module.lora_A.requires_grad = True\n",
    "        module.lora_B.requires_grad = True\n",
    "\n",
    "\n",
    "# Fine-tuning loop (example)\n",
    "prog_bar = tqdm(enumerate(train_data_loader))\n",
    "nepochs = 100\n",
    "epoch = 1\n",
    "while epoch <= nepochs:\n",
    "  #for step, (x, indiv_mels, mel, gt) in prog_bar:\n",
    "  for step, (x, indiv_mels, mel, gt) in enumerate(train_data_loader):\n",
    "    model.train()\n",
    "    # Your training loop here\n",
    "    optimizer.zero_grad()\n",
    "    g =  model(indiv_mels, x)\n",
    "    loss = recon_loss(g, gt)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Step: {}, Epoch: {}, Sync Loss: {}, L1: {}'.format(step, epoch, 0, loss.item()))\n",
    "\n",
    "  epoch += 1\n",
    "\n",
    "# Save LoRA adapter weights after fine-tuning\n",
    "torch.save(model.state_dict(), \"checkpoints/wav2lip_lora/lora_wav2lip_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your model\n",
    "model = Wav2Lip()\n",
    "\n",
    "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad],\n",
    "                           lr=0.001)\n",
    "\n",
    "load_wav2lip_checkpoint(\"checkpoints/checkpoint_step000136000.pth\", model, optimizer, reset_optimizer=True)\n",
    "\n",
    "# Load the LoRA fine-tuned weights\n",
    "lora_params = torch.load(\"checkpoints/wav2lip_lora/lora_wav2lip_weights.pth\")\n",
    "\n",
    "# Load the LoRA weights into the original model without changing the original weights\n",
    "model.load_state_dict(lora_params, strict=False)\n",
    "\n",
    "val_dataset = Dataset('val', 'training_data')\n",
    "val_data_loader = data_utils.DataLoader(val_dataset, batch_size=1, shuffle=True,num_workers=0)\n",
    "\n",
    "for x, indiv_mels, mel, gt in val_data_loader:\n",
    "            if x.shape[0] == 1:\n",
    "              step += 1\n",
    "              model.eval()\n",
    "\n",
    "              # Move data to CUDA device\n",
    "              x = x.to(device)\n",
    "              gt = gt.to(device)\n",
    "              indiv_mels = indiv_mels.to(device)\n",
    "              mel = mel.to(device)\n",
    "\n",
    "              g = model(indiv_mels, x)\n",
    "              \n",
    "              l1loss = recon_loss(g, gt)\n",
    "              print('The eval L1 loss', l1loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "for_wav2lip",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
